"""
Elasticsearch æ–‡æ¡£ç´¢å¼•å’Œæœç´¢è„šæœ¬

åŠŸèƒ½ï¼š
1. è¿æ¥åˆ° Elasticsearch
2. åˆ›å»ºç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
3. è§£æ docs æ–‡ä»¶å¤¹ä¸­çš„æ–‡æ¡£å¹¶åˆ†å—
4. å°†æ–‡æ¡£å—ç´¢å¼•åˆ° Elasticsearch
5. æ‰§è¡Œæœç´¢æŸ¥è¯¢
6. æ˜¾ç¤ºæœç´¢ç»“æœ

ä½¿ç”¨å‰å‡†å¤‡ï¼š
1. å®‰è£…ä¾èµ–ï¼š
   pip install elasticsearch -i https://pypi.tuna.tsinghua.edu.cn/simple
   pip install "qwen-agent[rag]" -i https://pypi.tuna.tsinghua.edu.cn/simple

2. å¦‚æœ ES ä¸åœ¨æœ¬åœ°ï¼Œè¯·ä¿®æ”¹ ES_HOST é…ç½®

3. ï¼ˆå¯é€‰ï¼‰å®‰è£… IK åˆ†è¯å™¨æ’ä»¶ä»¥è·å¾—æ›´å¥½çš„ä¸­æ–‡åˆ†è¯æ•ˆæœï¼š
   bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v8.x.x/elasticsearch-analysis-ik-8.x.x.zip
   æ³¨æ„ï¼šç‰ˆæœ¬å·éœ€è¦ä¸ ES ç‰ˆæœ¬åŒ¹é…

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-12-27
"""

import os
import json
from typing import List, Dict
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from qwen_agent.tools.doc_parser import DocParser
from qwen_agent.settings import DEFAULT_WORKSPACE

# ====== æ­¥éª¤ 1ï¼šé…ç½® Elasticsearch è¿æ¥ ======
# Elasticsearch è¿æ¥é…ç½®
# æ³¨æ„ï¼šå¦‚æœ ES ä¸åœ¨æœ¬åœ°ï¼Œè¯·ä¿®æ”¹ ES_HOST ä¸ºå®é™…çš„æœåŠ¡å™¨åœ°å€
ES_HOST = "localhost"  # ES æœåŠ¡å™¨åœ°å€ï¼Œæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼ˆå¦‚ï¼š192.168.1.100ï¼‰
ES_PORT = 9200  # ES ç«¯å£ï¼Œé»˜è®¤ 9200
ES_USERNAME = "elastic"  # ES ç”¨æˆ·å
ES_PASSWORD = "elastic"  # ES å¯†ç ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰

# ç´¢å¼•é…ç½®
INDEX_NAME = "pingan_employer_insurance"  # ç´¢å¼•åç§°

# æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„
DOCS_FOLDER = "./docs"  # æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„

# åˆ†è¯å™¨é…ç½®
# å¦‚æœ ES å®‰è£…äº† IK åˆ†è¯å™¨æ’ä»¶ï¼Œä½¿ç”¨ "ik_max_word"
# å¦‚æœæ²¡æœ‰å®‰è£… IK åˆ†è¯å™¨ï¼Œå°†ä½¿ç”¨ "standard"ï¼ˆæ ‡å‡†åˆ†è¯å™¨ï¼‰
USE_IK_ANALYZER = True  # æ˜¯å¦ä½¿ç”¨ IK åˆ†è¯å™¨ï¼ˆéœ€è¦å…ˆå®‰è£… IK æ’ä»¶ï¼‰


# ====== æ­¥éª¤ 2ï¼šè¿æ¥åˆ° Elasticsearch ======
def connect_elasticsearch():
    """
    è¿æ¥åˆ° Elasticsearch æœåŠ¡å™¨
    
    åŠŸèƒ½ï¼š
    - ä½¿ç”¨ç”¨æˆ·åå’Œå¯†ç è¿›è¡Œèº«ä»½éªŒè¯
    - æµ‹è¯•è¿æ¥æ˜¯å¦æˆåŠŸ
    
    è¿”å›ï¼š
    - Elasticsearch å®¢æˆ·ç«¯å¯¹è±¡
    """
    print("=" * 60)
    print("æ­¥éª¤ 1ï¼šè¿æ¥åˆ° Elasticsearch")
    print("=" * 60)
    
    # åˆ›å»º ES å®¢æˆ·ç«¯
    # ä½¿ç”¨ HTTP Basic Auth è¿›è¡Œèº«ä»½éªŒè¯
    # æ³¨æ„ï¼šelasticsearch 9.x ç‰ˆæœ¬ä½¿ç”¨ request_timeout è€Œä¸æ˜¯ timeout
    # å¯¹äº HTTPS è¿æ¥ï¼Œéœ€è¦ç¦ç”¨ SSL è¯ä¹¦éªŒè¯ï¼ˆé€‚ç”¨äºè‡ªç­¾åè¯ä¹¦ï¼‰
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)  # ç¦ç”¨ SSL è­¦å‘Š
    
    # å…ˆå°è¯• HTTPS è¿æ¥
    es_urls = [
        f"https://{ES_HOST}:{ES_PORT}",  # ä¼˜å…ˆå°è¯• HTTPS
        f"http://{ES_HOST}:{ES_PORT}"    # å¤‡ç”¨ HTTP
    ]
    
    last_error = None
    for es_url in es_urls:
        try:
            print(f"æ­£åœ¨å°è¯•è¿æ¥: {es_url}...")
            es = Elasticsearch(
                [es_url],
                basic_auth=(ES_USERNAME, ES_PASSWORD),  # 9.x ç‰ˆæœ¬ä½¿ç”¨ basic_auth è€Œä¸æ˜¯ http_auth
                request_timeout=30,  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
                max_retries=2,  # æœ€å¤§é‡è¯•æ¬¡æ•°
                retry_on_timeout=True,  # è¶…æ—¶æ—¶é‡è¯•
                verify_certs=False,  # ç¦ç”¨ SSL è¯ä¹¦éªŒè¯ï¼ˆé€‚ç”¨äºè‡ªç­¾åè¯ä¹¦ï¼‰
                ssl_show_warn=False  # ä¸æ˜¾ç¤º SSL è­¦å‘Š
            )
            
            # æµ‹è¯•è¿æ¥
            cluster_info = es.info()
            print(f"âœ… æˆåŠŸè¿æ¥åˆ° Elasticsearch: {es_url}")
            print(f"   é›†ç¾¤åç§°: {cluster_info['cluster_name']}")
            print(f"   ES ç‰ˆæœ¬: {cluster_info['version']['number']}")
            return es
        except Exception as e:
            last_error = e
            print(f"   âš ï¸  {es_url} è¿æ¥å¤±è´¥: {str(e)[:100]}")
            continue
    
    # å¦‚æœæ‰€æœ‰è¿æ¥éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªé”™è¯¯
    if last_error is None:
        last_error = Exception("æ— æ³•è¿æ¥åˆ° Elasticsearchï¼šæ‰€æœ‰è¿æ¥æ–¹å¼éƒ½å¤±è´¥")
    
    e = last_error
    error_msg = str(e)
    error_type = type(e).__name__
    print(f"âŒ è¿æ¥ Elasticsearch å¤±è´¥")
    print(f"   é”™è¯¯ç±»å‹: {error_type}")
    print(f"   é”™è¯¯ä¿¡æ¯: {error_msg}")
    print(f"\nå°è¯•è¿æ¥çš„åœ°å€: https://{ES_HOST}:{ES_PORT} å’Œ http://{ES_HOST}:{ES_PORT}")
    print(f"ç”¨æˆ·å: {ES_USERNAME}")
    print(f"å¯†ç : {'*' * len(ES_PASSWORD)} (å·²éšè—)")
    
    # å°è¯•æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè§£å†³æ–¹æ¡ˆ
    print("\n" + "=" * 60)
    print("è¯Šæ–­ä¿¡æ¯ï¼š")
    print("=" * 60)
    
    if "certificate" in error_msg.lower() or "ssl" in error_msg.lower():
        print("ğŸ’¡ SSL è¯ä¹¦ç›¸å…³é”™è¯¯")
        print("   å·²è‡ªåŠ¨ç¦ç”¨è¯ä¹¦éªŒè¯ï¼Œå¦‚æœä»æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ ES çš„ SSL é…ç½®")
    elif "authentication" in error_msg.lower() or "401" in error_msg or "403" in error_msg:
        print("ğŸ’¡ è®¤è¯å¤±è´¥")
        print("   è¯·æ£€æŸ¥ç”¨æˆ·åå’Œå¯†ç æ˜¯å¦æ­£ç¡®")
        print("   å¯ä»¥åœ¨æµè§ˆå™¨ä¸­è®¿é—® https://localhost:9200 æµ‹è¯•è®¤è¯")
    elif "connection" in error_msg.lower() or "refused" in error_msg.lower() or "timeout" in error_msg.lower():
        print("ğŸ’¡ è¿æ¥é—®é¢˜")
        print("   è¯·ç¡®è®¤ï¼š")
        print("   1. Elasticsearch æœåŠ¡å·²å¯åŠ¨")
        print("   2. ç«¯å£ 9200 æœªè¢«å…¶ä»–ç¨‹åºå ç”¨")
        print("   3. é˜²ç«å¢™æœªé˜»æ­¢è¿æ¥")
    elif "NameResolutionError" in error_type or "DNS" in error_msg:
        print("ğŸ’¡ DNS è§£æé—®é¢˜")
        print("   è¯·æ£€æŸ¥ä¸»æœºåœ°å€æ˜¯å¦æ­£ç¡®")
    else:
        print("ğŸ’¡ æœªçŸ¥é”™è¯¯")
        print("   è¯·æŸ¥çœ‹ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯")
    
    print("\nå»ºè®®çš„æ’æŸ¥æ­¥éª¤ï¼š")
    print("1. åœ¨æµè§ˆå™¨ä¸­è®¿é—® https://localhost:9200")
    print("   å¦‚æœæµè§ˆå™¨èƒ½è®¿é—®ï¼Œè¯´æ˜ ES æœåŠ¡æ­£å¸¸")
    print("2. æ£€æŸ¥ ES æ—¥å¿—æ–‡ä»¶ï¼ŒæŸ¥çœ‹æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯")
    print("3. ç¡®è®¤ç”¨æˆ·åå’Œå¯†ç æ˜¯å¦æ­£ç¡®")
    print("4. å°è¯•ä½¿ç”¨ curl å‘½ä»¤æµ‹è¯•è¿æ¥ï¼š")
    print(f'   curl -u {ES_USERNAME}:{ES_PASSWORD} -k https://localhost:9200')
    
    raise


# ====== æ­¥éª¤ 3ï¼šæ£€æŸ¥ IK åˆ†è¯å™¨æ˜¯å¦å¯ç”¨ ======
def check_ik_analyzer(es: Elasticsearch) -> bool:
    """
    æ£€æŸ¥ IK åˆ†è¯å™¨æ˜¯å¦å·²å®‰è£…
    
    å‚æ•°ï¼š
    - es: Elasticsearch å®¢æˆ·ç«¯å¯¹è±¡
    
    è¿”å›ï¼š
    - True: IK åˆ†è¯å™¨å¯ç”¨
    - False: IK åˆ†è¯å™¨ä¸å¯ç”¨
    """
    try:
        # å°è¯•åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç´¢å¼•æ¥æµ‹è¯• IK åˆ†è¯å™¨
        test_index = "_test_ik_analyzer"
        try:
            # å…ˆåˆ é™¤æµ‹è¯•ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if es.indices.exists(index=test_index):
                es.indices.delete(index=test_index)
            
            # åˆ›å»ºæµ‹è¯•ç´¢å¼•
            test_mapping = {
                "settings": {
                    "analysis": {
                        "analyzer": {
                            "test_ik": {
                                "type": "ik_max_word"
                            }
                        }
                    }
                }
            }
            # elasticsearch 9.x ç‰ˆæœ¬ï¼šç›´æ¥ä¼ é€’ settings å’Œ mappings å‚æ•°
            es.indices.create(
                index=test_index,
                settings=test_mapping["settings"],
                mappings=test_mapping.get("mappings", {})
            )
            es.indices.delete(index=test_index)
            return True
        except Exception:
            return False
    except Exception:
        return False


# ====== æ­¥éª¤ 4ï¼šåˆ›å»ºç´¢å¼• ======
def create_index(es: Elasticsearch, index_name: str):
    """
    åˆ›å»º Elasticsearch ç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    
    å‚æ•°ï¼š
    - es: Elasticsearch å®¢æˆ·ç«¯å¯¹è±¡
    - index_name: ç´¢å¼•åç§°
    
    åŠŸèƒ½ï¼š
    - å®šä¹‰ç´¢å¼•çš„æ˜ å°„ï¼ˆmappingï¼‰ç»“æ„
    - è®¾ç½®ä¸­æ–‡åˆ†è¯å™¨ï¼ˆik_max_word æˆ– standardï¼‰
    - åˆ›å»ºç´¢å¼•ï¼ˆå¦‚æœå·²å­˜åœ¨åˆ™è·³è¿‡ï¼‰
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 2ï¼šåˆ›å»ºç´¢å¼•")
    print("=" * 60)
    
    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
    if es.indices.exists(index=index_name):
        print(f"âš ï¸  ç´¢å¼• '{index_name}' å·²å­˜åœ¨ï¼Œå°†ä½¿ç”¨ç°æœ‰ç´¢å¼•")
        print("   å¦‚éœ€é‡æ–°åˆ›å»ºï¼Œè¯·å…ˆåˆ é™¤ç°æœ‰ç´¢å¼•")
        return
    
    # é€‰æ‹©åˆ†è¯å™¨
    # å¦‚æœé…ç½®ä½¿ç”¨ IK åˆ†è¯å™¨ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦å¯ç”¨
    if USE_IK_ANALYZER:
        ik_available = check_ik_analyzer(es)
        if ik_available:
            analyzer_name = "ik_max_word"
            search_analyzer_name = "ik_max_word"
            print(f"   âœ… IK åˆ†è¯å™¨å¯ç”¨ï¼Œä½¿ç”¨: ik_max_word")
        else:
            analyzer_name = "standard"
            search_analyzer_name = "standard"
            print(f"   âš ï¸  IK åˆ†è¯å™¨ä¸å¯ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°æ ‡å‡†åˆ†è¯å™¨")
            print(f"   æç¤º: å¦‚éœ€æ›´å¥½çš„ä¸­æ–‡åˆ†è¯æ•ˆæœï¼Œè¯·å®‰è£… IK åˆ†è¯å™¨æ’ä»¶")
            print(f"   å®‰è£…å‘½ä»¤: bin/elasticsearch-plugin install analysis-ik")
    else:
        analyzer_name = "standard"
        search_analyzer_name = "standard"
        print(f"   ä½¿ç”¨åˆ†è¯å™¨: æ ‡å‡†åˆ†è¯å™¨ (standard)")
    
    index_mapping = {
        "settings": {
            # è®¾ç½®åˆ†ç‰‡å’Œå‰¯æœ¬æ•°
            "number_of_shards": 1,  # ä¸»åˆ†ç‰‡æ•°
            "number_of_replicas": 0,  # å‰¯æœ¬æ•°ï¼ˆå¼€å‘ç¯å¢ƒå¯ä»¥è®¾ä¸º 0ï¼‰
        },
        "mappings": {
            "properties": {
                # æ–‡æ¡£æ ‡é¢˜å­—æ®µ
                "title": {
                    "type": "text",  # æ–‡æœ¬ç±»å‹ï¼Œæ”¯æŒå…¨æ–‡æœç´¢
                    "analyzer": analyzer_name,  # ä½¿ç”¨æŒ‡å®šçš„åˆ†è¯å™¨
                    "search_analyzer": search_analyzer_name,  # æœç´¢æ—¶ä¹Ÿä½¿ç”¨æŒ‡å®šçš„åˆ†è¯å™¨
                    "fields": {
                        "keyword": {
                            "type": "keyword"  # ä¿ç•™åŸå§‹å€¼ï¼Œç”¨äºç²¾ç¡®åŒ¹é…
                        }
                    }
                },
                # æ–‡æ¡£æºæ–‡ä»¶è·¯å¾„
                "source": {
                    "type": "keyword"  # å…³é”®å­—ç±»å‹ï¼Œä¸è¿›è¡Œåˆ†è¯
                },
                # æ–‡æ¡£å—å†…å®¹ï¼ˆä¸»è¦æœç´¢å­—æ®µï¼‰
                "content": {
                    "type": "text",
                    "analyzer": analyzer_name,
                    "search_analyzer": search_analyzer_name
                },
                # å— IDï¼ˆåœ¨åŒä¸€æ–‡æ¡£ä¸­çš„åºå·ï¼‰
                "chunk_id": {
                    "type": "integer"
                },
                # é¡µç ï¼ˆå¦‚æœæ–‡æ¡£æœ‰åˆ†é¡µï¼‰
                "page_num": {
                    "type": "integer"
                },
                # Token æ•°é‡
                "token_count": {
                    "type": "integer"
                },
                # æ–‡æ¡£ç±»å‹ï¼ˆtxt, pdf, docx ç­‰ï¼‰
                "file_type": {
                    "type": "keyword"
                }
            }
        }
    }
    
    try:
        # åˆ›å»ºç´¢å¼•
        # elasticsearch 9.x ç‰ˆæœ¬ï¼šç›´æ¥ä¼ é€’ settings å’Œ mappings å‚æ•°
        es.indices.create(
            index=index_name,
            settings=index_mapping["settings"],
            mappings=index_mapping["mappings"]
        )
        print(f"âœ… æˆåŠŸåˆ›å»ºç´¢å¼•: {index_name}")
        print(f"   ç´¢å¼•æ˜ å°„å·²é…ç½®")
    except Exception as e:
        print(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {str(e)}")
        raise


# ====== æ­¥éª¤ 4ï¼šè§£ææ–‡æ¡£å¹¶åˆ†å— ======
def parse_and_chunk_documents(docs_folder: str) -> List[Dict]:
    """
    è§£æ docs æ–‡ä»¶å¤¹ä¸­çš„æ–‡æ¡£å¹¶åˆ†å—
    
    å‚æ•°ï¼š
    - docs_folder: æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„
    
    è¿”å›ï¼š
    - æ–‡æ¡£å—åˆ—è¡¨ï¼Œæ¯ä¸ªå—åŒ…å« title, source, content, chunk_id ç­‰ä¿¡æ¯
    
    åŠŸèƒ½ï¼š
    - éå† docs æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    - ä½¿ç”¨ DocParser è§£ææ–‡æ¡£
    - å°†æ–‡æ¡£åˆ†å—ï¼ˆchunkï¼‰
    - è¿”å›æ‰€æœ‰æ–‡æ¡£å—
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 3ï¼šè§£ææ–‡æ¡£å¹¶åˆ†å—")
    print("=" * 60)
    
    # åˆ›å»º DocParser å®ä¾‹
    # é…ç½®åˆ†å—å¤§å°ï¼šæ¯ä¸ªå—æœ€å¤š 500 tokens
    doc_parser = DocParser({
        'max_ref_token': 20000,  # æœ€å¤§å‚è€ƒ token æ•°
        'parser_page_size': 500  # æ¯ä¸ªå—çš„æœ€å¤§ token æ•°
    })
    
    # è·å–æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
    if not os.path.exists(docs_folder):
        raise FileNotFoundError(f"æ–‡æ¡£æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {docs_folder}")
    
    # è·å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    files = []
    for file in os.listdir(docs_folder):
        file_path = os.path.join(docs_folder, file)
        if os.path.isfile(file_path):  # ç¡®ä¿æ˜¯æ–‡ä»¶è€Œä¸æ˜¯ç›®å½•
            files.append(file_path)
    
    print(f"ğŸ“ æ‰¾åˆ° {len(files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
    
    # è§£ææ‰€æœ‰æ–‡æ¡£
    all_chunks = []
    for i, file_path in enumerate(files, 1):
        print(f"\n[{i}/{len(files)}] æ­£åœ¨è§£æ: {os.path.basename(file_path)}")
        
        try:
            # ä½¿ç”¨ DocParser è§£ææ–‡æ¡£
            # è¿™ä¼šè‡ªåŠ¨å¤„ç†æ–‡æ¡£åˆ†å—å’Œç¼“å­˜
            record = doc_parser.call(params={'url': file_path})
            
            # record ç»“æ„ï¼š
            # {
            #   'url': æ–‡ä»¶è·¯å¾„,
            #   'title': æ–‡æ¡£æ ‡é¢˜,
            #   'raw': [
            #     {
            #       'content': å—å†…å®¹,
            #       'metadata': {'source': æºæ–‡ä»¶, 'title': æ ‡é¢˜, 'chunk_id': å—ID},
            #       'token': token æ•°é‡
            #     },
            #     ...
            #   ]
            # }
            
            # è·å–æ–‡ä»¶æ‰©å±•åï¼ˆæ–‡ä»¶ç±»å‹ï¼‰
            file_ext = os.path.splitext(file_path)[1].lower().lstrip('.')
            
            # å¤„ç†æ¯ä¸ªæ–‡æ¡£å—
            for chunk in record['raw']:
                chunk_data = {
                    'title': record['title'],  # æ–‡æ¡£æ ‡é¢˜
                    'source': file_path,  # æºæ–‡ä»¶è·¯å¾„
                    'content': chunk['content'],  # å—å†…å®¹
                    'chunk_id': chunk['metadata'].get('chunk_id', 0),  # å— ID
                    'page_num': chunk['metadata'].get('page_num', 1),  # é¡µç ï¼ˆå¦‚æœæœ‰ï¼‰
                    'token_count': chunk['token'],  # Token æ•°é‡
                    'file_type': file_ext  # æ–‡ä»¶ç±»å‹
                }
                all_chunks.append(chunk_data)
            
            print(f"   âœ… è§£æå®Œæˆï¼Œç”Ÿæˆ {len(record['raw'])} ä¸ªæ–‡æ¡£å—")
            
        except Exception as e:
            print(f"   âŒ è§£æå¤±è´¥: {str(e)}")
            print(f"   è·³è¿‡æ­¤æ–‡ä»¶ï¼Œç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶")
            continue
    
    print(f"\nâœ… æ–‡æ¡£è§£æå®Œæˆï¼")
    print(f"   æ€»å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
    
    return all_chunks


# ====== æ­¥éª¤ 5ï¼šç´¢å¼•æ–‡æ¡£åˆ° Elasticsearch ======
def index_documents(es: Elasticsearch, index_name: str, chunks: List[Dict]):
    """
    å°†æ–‡æ¡£å—ç´¢å¼•åˆ° Elasticsearch
    
    å‚æ•°ï¼š
    - es: Elasticsearch å®¢æˆ·ç«¯å¯¹è±¡
    - index_name: ç´¢å¼•åç§°
    - chunks: æ–‡æ¡£å—åˆ—è¡¨
    
    åŠŸèƒ½ï¼š
    - ä½¿ç”¨ bulk API æ‰¹é‡ç´¢å¼•æ–‡æ¡£
    - ä¸ºæ¯ä¸ªæ–‡æ¡£å—ç”Ÿæˆå”¯ä¸€ ID
    - æ˜¾ç¤ºç´¢å¼•è¿›åº¦
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 4ï¼šç´¢å¼•æ–‡æ¡£åˆ° Elasticsearch")
    print("=" * 60)
    
    if not chunks:
        print("âš ï¸  æ²¡æœ‰æ–‡æ¡£å—éœ€è¦ç´¢å¼•")
        return
    
    # å‡†å¤‡æ‰¹é‡ç´¢å¼•çš„æ•°æ®
    actions = []
    for i, chunk in enumerate(chunks):
        # ç”Ÿæˆæ–‡æ¡£ IDï¼šä½¿ç”¨æºæ–‡ä»¶è·¯å¾„å’Œå— ID çš„ç»„åˆ
        doc_id = f"{hash(chunk['source'])}_{chunk['chunk_id']}"
        
        # æ„å»ºè¦ç´¢å¼•çš„æ–‡æ¡£
        action = {
            "_index": index_name,  # ç´¢å¼•åç§°
            "_id": doc_id,  # æ–‡æ¡£ ID
            "_source": chunk  # æ–‡æ¡£å†…å®¹
        }
        actions.append(action)
    
    print(f"ğŸ“¤ å‡†å¤‡ç´¢å¼• {len(actions)} ä¸ªæ–‡æ¡£å—...")
    
    try:
        # ä½¿ç”¨ bulk API æ‰¹é‡ç´¢å¼•
        # bulk å‡½æ•°ä¼šè‡ªåŠ¨å¤„ç†æ‰¹é‡æ“ä½œï¼Œæé«˜æ•ˆç‡
        success_count, failed_items = bulk(es, actions, chunk_size=100, request_timeout=60)
        
        print(f"âœ… ç´¢å¼•å®Œæˆï¼")
        print(f"   æˆåŠŸç´¢å¼•: {success_count} ä¸ªæ–‡æ¡£")
        if failed_items:
            print(f"   å¤±è´¥: {len(failed_items)} ä¸ªæ–‡æ¡£")
            for item in failed_items[:5]:  # åªæ˜¾ç¤ºå‰ 5 ä¸ªå¤±è´¥é¡¹
                print(f"      - {item}")
        
        # åˆ·æ–°ç´¢å¼•ï¼Œä½¿æ–°ç´¢å¼•çš„æ–‡æ¡£ç«‹å³å¯æœç´¢
        es.indices.refresh(index=index_name)
        print(f"   ç´¢å¼•å·²åˆ·æ–°ï¼Œæ–‡æ¡£å¯ç«‹å³æœç´¢")
        
    except Exception as e:
        print(f"âŒ ç´¢å¼•å¤±è´¥: {str(e)}")
        raise


# ====== æ­¥éª¤ 6ï¼šæ‰§è¡Œæœç´¢ ======
def search_documents(es: Elasticsearch, index_name: str, search_query: str, top_k: int = 10):
    """
    åœ¨ Elasticsearch ä¸­æœç´¢æ–‡æ¡£
    
    å‚æ•°ï¼š
    - es: Elasticsearch å®¢æˆ·ç«¯å¯¹è±¡
    - index_name: ç´¢å¼•åç§°
    - search_query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
    - top_k: è¿”å›å‰ K ä¸ªç»“æœï¼Œé»˜è®¤ 10
    
    è¿”å›ï¼š
    - æœç´¢ç»“æœåˆ—è¡¨
    
    åŠŸèƒ½ï¼š
    - ä½¿ç”¨å¤šå­—æ®µæœç´¢ï¼ˆtitle å’Œ contentï¼‰
    - ä½¿ç”¨ä¸­æ–‡åˆ†è¯å™¨è¿›è¡Œæœç´¢
    - è¿”å›ç›¸å…³æ€§è¯„åˆ†æœ€é«˜çš„æ–‡æ¡£
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 5ï¼šæ‰§è¡Œæœç´¢")
    print("=" * 60)
    
    print(f"ğŸ” æœç´¢æŸ¥è¯¢: {search_query}")
    print(f"   è¿”å›å‰ {top_k} ä¸ªç»“æœ")
    
    # æ„å»ºæœç´¢æŸ¥è¯¢
    # ä½¿ç”¨ multi_match æŸ¥è¯¢ï¼Œåœ¨ title å’Œ content å­—æ®µä¸­æœç´¢
    search_body = {
        "query": {
            "multi_match": {
                "query": search_query,  # æœç´¢æŸ¥è¯¢
                "fields": ["title^2", "content"],  # æœç´¢å­—æ®µï¼Œtitle æƒé‡ä¸º 2ï¼ˆæ›´é‡è¦ï¼‰
                "type": "best_fields"  # æœ€ä½³å­—æ®µåŒ¹é…
            }
        },
        "highlight": {  # é«˜äº®æ˜¾ç¤ºåŒ¹é…çš„æ–‡æœ¬
            "fields": {
                "title": {},
                "content": {
                    "fragment_size": 200,  # ç‰‡æ®µå¤§å°
                    "number_of_fragments": 3  # è¿”å›çš„ç‰‡æ®µæ•°
                }
            },
            "pre_tags": ["<mark>"],  # é«˜äº®å¼€å§‹æ ‡ç­¾
            "post_tags": ["</mark>"]  # é«˜äº®ç»“æŸæ ‡ç­¾
        },
        "size": top_k  # è¿”å›ç»“æœæ•°é‡
    }
    
    try:
        # æ‰§è¡Œæœç´¢
        # elasticsearch 9.x ç‰ˆæœ¬ï¼šç›´æ¥ä¼ é€’ queryã€highlight å’Œ size å‚æ•°
        response = es.search(
            index=index_name,
            query=search_body["query"],
            highlight=search_body["highlight"],
            size=search_body["size"]
        )
        
        # è§£ææœç´¢ç»“æœ
        hits = response['hits']['hits']
        total = response['hits']['total']['value']
        
        print(f"\nâœ… æœç´¢å®Œæˆï¼")
        print(f"   æ‰¾åˆ° {total} ä¸ªç›¸å…³æ–‡æ¡£")
        print(f"   è¿”å›å‰ {len(hits)} ä¸ªç»“æœ\n")
        
        return hits
        
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
        raise


# ====== æ­¥éª¤ 7ï¼šæ˜¾ç¤ºæœç´¢ç»“æœ ======
def display_search_results(hits: List[Dict]):
    """
    æ ¼å¼åŒ–å¹¶æ˜¾ç¤ºæœç´¢ç»“æœ
    
    å‚æ•°ï¼š
    - hits: æœç´¢ç»“æœåˆ—è¡¨
    
    åŠŸèƒ½ï¼š
    - æ˜¾ç¤ºæ¯ä¸ªç»“æœçš„æ ‡é¢˜ã€æ¥æºã€ç›¸å…³æ€§è¯„åˆ†
    - æ˜¾ç¤ºé«˜äº®çš„å†…å®¹ç‰‡æ®µ
    - æ ¼å¼åŒ–è¾“å‡ºï¼Œä¾¿äºé˜…è¯»
    """
    print("=" * 60)
    print("æ­¥éª¤ 6ï¼šæ˜¾ç¤ºæœç´¢ç»“æœ")
    print("=" * 60)
    
    if not hits:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        return
    
    # éå†æ˜¾ç¤ºæ¯ä¸ªæœç´¢ç»“æœ
    for i, hit in enumerate(hits, 1):
        score = hit['_score']  # ç›¸å…³æ€§è¯„åˆ†
        source = hit['_source']  # æ–‡æ¡£å†…å®¹
        highlight = hit.get('highlight', {})  # é«˜äº®ç‰‡æ®µ
        
        print(f"\n{'=' * 60}")
        print(f"ç»“æœ {i} (ç›¸å…³æ€§è¯„åˆ†: {score:.4f})")
        print(f"{'=' * 60}")
        
        # æ˜¾ç¤ºæ ‡é¢˜ï¼ˆå¦‚æœæœ‰é«˜äº®ï¼Œæ˜¾ç¤ºé«˜äº®ç‰ˆæœ¬ï¼‰
        title = highlight.get('title', [source.get('title', 'æ— æ ‡é¢˜')])[0]
        print(f"ğŸ“„ æ ‡é¢˜: {title}")
        
        # æ˜¾ç¤ºæ¥æºæ–‡ä»¶
        source_file = os.path.basename(source.get('source', 'æœªçŸ¥'))
        print(f"ğŸ“ æ¥æº: {source_file}")
        
        # æ˜¾ç¤ºæ–‡ä»¶ç±»å‹å’Œå—ä¿¡æ¯
        print(f"ğŸ“Š ä¿¡æ¯: æ–‡ä»¶ç±»å‹={source.get('file_type', 'æœªçŸ¥')}, "
              f"å—ID={source.get('chunk_id', 0)}, "
              f"Tokenæ•°={source.get('token_count', 0)}")
        
        # æ˜¾ç¤ºé«˜äº®çš„å†…å®¹ç‰‡æ®µ
        if 'content' in highlight:
            print(f"\nğŸ’¡ ç›¸å…³å†…å®¹ç‰‡æ®µ:")
            for fragment in highlight['content'][:3]:  # æœ€å¤šæ˜¾ç¤º 3 ä¸ªç‰‡æ®µ
                # ç§»é™¤ HTML æ ‡ç­¾ä»¥ä¾¿åœ¨ç»ˆç«¯æ˜¾ç¤ºï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä¿ç•™ï¼‰
                fragment_text = fragment.replace('<mark>', 'ã€').replace('</mark>', 'ã€‘')
                print(f"   {fragment_text}")
        else:
            # å¦‚æœæ²¡æœ‰é«˜äº®ï¼Œæ˜¾ç¤ºåŸå§‹å†…å®¹çš„å‰ 300 ä¸ªå­—ç¬¦
            content = source.get('content', '')
            preview = content[:300] + ('...' if len(content) > 300 else '')
            print(f"\nğŸ’¡ å†…å®¹é¢„è§ˆ:")
            print(f"   {preview}")
        
        print()
    
    print("=" * 60)


# ====== ä¸»ç¨‹åº ======
def main():
    """
    ä¸»ç¨‹åºï¼šæ‰§è¡Œå®Œæ•´çš„ç´¢å¼•å’Œæœç´¢æµç¨‹
    
    æµç¨‹ï¼š
    1. è¿æ¥åˆ° Elasticsearch
    2. åˆ›å»ºç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    3. è§£ææ–‡æ¡£å¹¶åˆ†å—
    4. ç´¢å¼•æ–‡æ¡£åˆ° Elasticsearch
    5. æ‰§è¡Œæœç´¢
    6. æ˜¾ç¤ºæœç´¢ç»“æœ
    """
    print("\n" + "ğŸš€" * 30)
    print("Elasticsearch æ–‡æ¡£ç´¢å¼•å’Œæœç´¢ç³»ç»Ÿ")
    print("ğŸš€" * 30 + "\n")
    
    try:
        # æ­¥éª¤ 1ï¼šè¿æ¥åˆ° Elasticsearch
        es = connect_elasticsearch()
        
        # æ­¥éª¤ 2ï¼šåˆ›å»ºç´¢å¼•
        create_index(es, INDEX_NAME)
        
        # æ­¥éª¤ 3ï¼šè§£ææ–‡æ¡£å¹¶åˆ†å—
        chunks = parse_and_chunk_documents(DOCS_FOLDER)
        
        # æ­¥éª¤ 4ï¼šç´¢å¼•æ–‡æ¡£
        if chunks:
            index_documents(es, INDEX_NAME, chunks)
        else:
            print("âš ï¸  æ²¡æœ‰æ–‡æ¡£å—éœ€è¦ç´¢å¼•ï¼Œè·³è¿‡ç´¢å¼•æ­¥éª¤")
        
        # æ­¥éª¤ 5ï¼šæ‰§è¡Œæœç´¢
        search_query = "å·¥ä¼¤ä¿é™©å’Œé›‡ä¸»é™©æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
        hits = search_documents(es, INDEX_NAME, search_query, top_k=10)
        
        # æ­¥éª¤ 6ï¼šæ˜¾ç¤ºæœç´¢ç»“æœ
        display_search_results(hits)
        
        print("\n" + "âœ…" * 30)
        print("æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆï¼")
        print("âœ…" * 30 + "\n")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


# ====== ç¨‹åºå…¥å£ ======
if __name__ == '__main__':
    exit(main())

