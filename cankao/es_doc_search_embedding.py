"""
åŸºäº Embedding å‘é‡çš„ Elasticsearch æ–‡æ¡£ç´¢å¼•å’Œæœç´¢è„šæœ¬

åŠŸèƒ½ï¼š
1. è¿æ¥åˆ° Elasticsearch
2. åˆ›å»ºæ”¯æŒå‘é‡æœç´¢çš„ç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
3. è§£æ docs æ–‡ä»¶å¤¹ä¸­çš„æ–‡æ¡£å¹¶åˆ†å—
4. ä½¿ç”¨ text-embedding-v4 ç”Ÿæˆæ–‡æ¡£å‘é‡
5. å°†æ–‡æ¡£å—å’Œå‘é‡ç´¢å¼•åˆ° Elasticsearch
6. ä½¿ç”¨å‘é‡æœç´¢æ‰§è¡ŒæŸ¥è¯¢
7. æ˜¾ç¤ºæœç´¢ç»“æœ

ä½¿ç”¨å‰å‡†å¤‡ï¼š
1. å®‰è£…ä¾èµ–ï¼š
   pip install elasticsearch openai -i https://pypi.tuna.tsinghua.edu.cn/simple
   pip install "qwen-agent[rag]" -i https://pypi.tuna.tsinghua.edu.cn/simple

2. è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
   export DASHSCOPE_API_KEY=your_api_key

3. å¦‚æœ ES ä¸åœ¨æœ¬åœ°ï¼Œè¯·ä¿®æ”¹ ES_HOST é…ç½®

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2025-12-27
"""

import os
import json
import hashlib
from typing import List, Dict, Optional
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from qwen_agent.tools.doc_parser import DocParser
from openai import OpenAI

# ====== æ­¥éª¤ 1ï¼šé…ç½® Elasticsearch è¿æ¥ ======
# Elasticsearch è¿æ¥é…ç½®
ES_HOST = "localhost"  # ES æœåŠ¡å™¨åœ°å€
ES_PORT = 9200  # ES ç«¯å£
ES_USERNAME = "elastic"  # ES ç”¨æˆ·å
ES_PASSWORD = "elastic"  # ES å¯†ç ï¼ˆå·²æ›´æ–°ä¸º elasticï¼‰
# æ³¨æ„ï¼šå¦‚æœ ES ä½¿ç”¨ HTTPSï¼Œè¯·ç¡®ä¿å¯†ç æ­£ç¡®

# ç´¢å¼•é…ç½®
INDEX_NAME = "pingan_employer_insurance_embedding"  # ç´¢å¼•åç§°ï¼ˆä½¿ç”¨ä¸åŒçš„ç´¢å¼•åé¿å…å†²çªï¼‰

# æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„
DOCS_FOLDER = "./docs"  # æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„

# Embedding é…ç½®
EMBEDDING_MODEL = "text-embedding-v4"  # ä½¿ç”¨ text-embedding-v4 æ¨¡å‹
EMBEDDING_DIMENSIONS = 1024  # å‘é‡ç»´åº¦ï¼ˆtext-embedding-v4 æ”¯æŒ 1024 ç»´ï¼‰
DASHSCOPE_API_KEY = os.getenv('DASHSCOPE_API_KEY', '')  # ä»ç¯å¢ƒå˜é‡è·å– API Key


# ====== æ­¥éª¤ 2ï¼šåˆå§‹åŒ– Embedding å®¢æˆ·ç«¯ ======
def init_embedding_client():
    """
    åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äºè°ƒç”¨ text-embedding-v4ï¼‰
    
    è¿”å›ï¼š
    - OpenAI å®¢æˆ·ç«¯å¯¹è±¡
    """
    if not DASHSCOPE_API_KEY:
        raise ValueError("è¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
    
    client = OpenAI(
        api_key=DASHSCOPE_API_KEY,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"  # ç™¾ç‚¼æœåŠ¡çš„ base_url
    )
    return client


# ====== æ­¥éª¤ 3ï¼šç”Ÿæˆæ–‡æœ¬å‘é‡ ======
def generate_embedding(text: str, client: OpenAI) -> List[float]:
    """
    ä½¿ç”¨ text-embedding-v4 ç”Ÿæˆæ–‡æœ¬çš„å‘é‡åµŒå…¥
    
    å‚æ•°ï¼š
    - text: è¦ç”Ÿæˆå‘é‡çš„æ–‡æœ¬
    - client: OpenAI å®¢æˆ·ç«¯å¯¹è±¡
    
    è¿”å›ï¼š
    - å‘é‡åˆ—è¡¨ï¼ˆæµ®ç‚¹æ•°åˆ—è¡¨ï¼‰
    
    åŠŸèƒ½ï¼š
    - è°ƒç”¨ text-embedding-v4 æ¨¡å‹
    - è¿”å› 1024 ç»´çš„å‘é‡
    """
    try:
        # è°ƒç”¨ embedding API
        response = client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=text,
            dimensions=EMBEDDING_DIMENSIONS,  # æŒ‡å®šå‘é‡ç»´åº¦
            encoding_format="float"  # è¿”å›æµ®ç‚¹æ•°æ ¼å¼
        )
        
        # æå–å‘é‡
        embedding = response.data[0].embedding
        return embedding
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå‘é‡å¤±è´¥: {str(e)}")
        raise


# ====== æ­¥éª¤ 4ï¼šè¿æ¥åˆ° Elasticsearch ======
def connect_elasticsearch():
    """
    è¿æ¥åˆ° Elasticsearch æœåŠ¡å™¨
    
    åŠŸèƒ½ï¼š
    - ä½¿ç”¨ç”¨æˆ·åå’Œå¯†ç è¿›è¡Œèº«ä»½éªŒè¯
    - æµ‹è¯•è¿æ¥æ˜¯å¦æˆåŠŸ
    - æä¾›è¯¦ç»†çš„é”™è¯¯è¯Šæ–­
    
    è¿”å›ï¼š
    - Elasticsearch å®¢æˆ·ç«¯å¯¹è±¡
    """
    print("=" * 60)
    print("æ­¥éª¤ 1ï¼šè¿æ¥åˆ° Elasticsearch")
    print("=" * 60)
    
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # å…ˆæ£€æŸ¥ ES æœåŠ¡æ˜¯å¦å¯è¾¾ï¼ˆç®€å•æµ‹è¯•ï¼‰
    import socket
    print("ğŸ” æ£€æŸ¥ ES æœåŠ¡çŠ¶æ€...")
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(3)
        result = sock.connect_ex((ES_HOST, ES_PORT))
        sock.close()
        if result != 0:
            print(f"âŒ æ— æ³•è¿æ¥åˆ° {ES_HOST}:{ES_PORT}ï¼Œç«¯å£å¯èƒ½æœªå¼€æ”¾")
            print("   è¯·ç¡®è®¤ Elasticsearch æœåŠ¡å·²å¯åŠ¨")
            print("   å¯åŠ¨å‘½ä»¤: cd D:\\Software\\elasticsearch-9.2.3\\bin && elasticsearch.bat")
            raise Exception(f"ES æœåŠ¡æœªè¿è¡Œæˆ–ç«¯å£ {ES_PORT} æœªå¼€æ”¾")
        else:
            print(f"âœ… ç«¯å£ {ES_PORT} å¯è¾¾ï¼ŒES æœåŠ¡å¯èƒ½æ­£åœ¨è¿è¡Œ")
    except Exception as e:
        if "ES æœåŠ¡æœªè¿è¡Œ" in str(e):
            raise
        print(f"âš ï¸  ç½‘ç»œæ£€æŸ¥å¤±è´¥: {str(e)}")
    
    # å°è¯•ä½¿ç”¨ requests ç›´æ¥æµ‹è¯•è¿æ¥ï¼ˆæ›´ç®€å•çš„æ–¹å¼ï¼‰
    print("\nğŸ” ä½¿ç”¨ requests æµ‹è¯•è¿æ¥...")
    try:
        import requests
        from requests.auth import HTTPBasicAuth
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # æµ‹è¯• HTTPS è¿æ¥
        test_urls = [
            f"https://{ES_HOST}:{ES_PORT}",
            f"http://{ES_HOST}:{ES_PORT}"
        ]
        
        for test_url in test_urls:
            try:
                print(f"   æµ‹è¯• {test_url}...", end='', flush=True)
                response = requests.get(
                    test_url,
                    auth=HTTPBasicAuth(ES_USERNAME, ES_PASSWORD),
                    verify=False,  # ç¦ç”¨ SSL éªŒè¯
                    timeout=10
                )
                if response.status_code == 200:
                    print(f" âœ… æˆåŠŸï¼")
                    print(f"   å“åº”: {response.json().get('cluster_name', 'æœªçŸ¥')}")
                    break
                else:
                    print(f" âŒ HTTP {response.status_code}")
            except requests.exceptions.SSLError as e:
                print(f" âš ï¸  SSL é”™è¯¯ï¼ˆç»§ç»­å°è¯•å…¶ä»–æ–¹å¼ï¼‰")
            except Exception as e:
                print(f" âŒ å¤±è´¥: {str(e)[:50]}")
    except ImportError:
        print("   âš ï¸  requests åº“æœªå®‰è£…ï¼Œè·³è¿‡ç›´æ¥æµ‹è¯•")
    except Exception as e:
        print(f"   âš ï¸  æµ‹è¯•å¤±è´¥: {str(e)}")
    
    print()
    
    # å°è¯• HTTPS å’Œ HTTP è¿æ¥
    es_urls = [
        f"https://{ES_HOST}:{ES_PORT}",  # ä¼˜å…ˆå°è¯• HTTPSï¼ˆå› ä¸ºç”¨æˆ·è¯´å¯ä»¥è®¿é—® https://localhost:9200ï¼‰
        f"http://{ES_HOST}:{ES_PORT}"     # å¤‡ç”¨ HTTP
    ]
    
    last_error = None
    for es_url in es_urls:
        try:
            print(f"æ­£åœ¨å°è¯•è¿æ¥: {es_url}...")
            
            # é…ç½® SSL ä¸Šä¸‹æ–‡ï¼Œå®Œå…¨ç¦ç”¨è¯ä¹¦éªŒè¯ï¼ˆé€‚ç”¨äºè‡ªç­¾åè¯ä¹¦ï¼‰
            import ssl
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            
            # æ„å»º ES å®¢æˆ·ç«¯é…ç½®
            es_config = {
                'basic_auth': (ES_USERNAME, ES_PASSWORD) if ES_USERNAME and ES_PASSWORD else None,
                'request_timeout': 60,
                'max_retries': 1,
                'retry_on_timeout': False,
                'verify_certs': False,
                'ssl_show_warn': False,
                'connections_per_node': 1,
                'http_compress': True
            }
            
            # å¦‚æœæ˜¯ HTTPSï¼Œæ·»åŠ  SSL ä¸Šä¸‹æ–‡
            if es_url.startswith('https'):
                es_config['ssl_context'] = ssl_context
            
            es = Elasticsearch([es_url], **es_config)
            
            # æµ‹è¯•è¿æ¥ï¼ˆä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶ï¼‰
            cluster_info = es.info(request_timeout=10)
            print(f"âœ… æˆåŠŸè¿æ¥åˆ° Elasticsearch: {es_url}")
            print(f"   é›†ç¾¤åç§°: {cluster_info['cluster_name']}")
            print(f"   ES ç‰ˆæœ¬: {cluster_info['version']['number']}")
            return es
        except Exception as e:
            last_error = e
            error_msg = str(e)
            error_type = type(e).__name__
            
            # æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´è¯¦ç»†çš„è¯Šæ–­
            if "RemoteDisconnected" in error_type or "Connection aborted" in error_msg:
                print(f"   âš ï¸  {es_url} è¿æ¥è¢«è¿œç¨‹ç«¯å…³é—­")
                print(f"      å¯èƒ½åŸå› ï¼š")
                print(f"      1. ES æœåŠ¡æœªå®Œå…¨å¯åŠ¨")
                print(f"      2. è®¤è¯å¤±è´¥ï¼ˆç”¨æˆ·åæˆ–å¯†ç é”™è¯¯ï¼‰")
                print(f"      3. ES é…ç½®äº†å®‰å…¨ç­–ç•¥ï¼Œæ‹’ç»è¿æ¥")
            elif "timeout" in error_msg.lower():
                print(f"   âš ï¸  {es_url} è¿æ¥è¶…æ—¶")
                print(f"      å¯èƒ½åŸå› ï¼šES æœåŠ¡æœªå¯åŠ¨æˆ–ç½‘ç»œé—®é¢˜")
            elif "401" in error_msg or "403" in error_msg or "authentication" in error_msg.lower():
                print(f"   âš ï¸  {es_url} è®¤è¯å¤±è´¥")
                print(f"      è¯·æ£€æŸ¥ç”¨æˆ·åå’Œå¯†ç æ˜¯å¦æ­£ç¡®")
            else:
                print(f"   âš ï¸  {es_url} è¿æ¥å¤±è´¥: {error_msg[:150]}")
            continue
    
    # å¦‚æœæ‰€æœ‰è¿æ¥éƒ½å¤±è´¥ï¼Œæä¾›è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
    if last_error:
        print(f"\nâŒ è¿æ¥ Elasticsearch å¤±è´¥")
        print(f"   é”™è¯¯ç±»å‹: {type(last_error).__name__}")
        print(f"   é”™è¯¯ä¿¡æ¯: {str(last_error)[:200]}")
        print(f"\nå°è¯•è¿æ¥çš„åœ°å€:")
        for es_url in es_urls:
            print(f"   - {es_url}")
        print(f"\nç”¨æˆ·å: {ES_USERNAME}")
        print(f"å¯†ç : {'*' * len(ES_PASSWORD) if ES_PASSWORD else '(æœªè®¾ç½®)'}")
        
        print("\n" + "=" * 60)
        print("è¯Šæ–­å»ºè®®ï¼š")
        print("=" * 60)
        print("1. ç¡®è®¤ Elasticsearch æœåŠ¡å·²å¯åŠ¨")
        print("   æ£€æŸ¥æ–¹æ³•ï¼šåœ¨æµè§ˆå™¨è®¿é—® https://localhost:9200")
        print("2. æ£€æŸ¥ç”¨æˆ·åå’Œå¯†ç æ˜¯å¦æ­£ç¡®")
        print("   å¯ä»¥åœ¨æµè§ˆå™¨ä¸­æµ‹è¯•è®¤è¯")
        print("3. æ£€æŸ¥ ES æ—¥å¿—æ–‡ä»¶ï¼ŒæŸ¥çœ‹æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯")
        print("4. å°è¯•ä½¿ç”¨ curl å‘½ä»¤æµ‹è¯•è¿æ¥ï¼š")
        print(f'   curl -u {ES_USERNAME}:{ES_PASSWORD} -k https://localhost:9200')
        print("5. å¦‚æœ ES ä½¿ç”¨ HTTPï¼Œç¡®ä¿è„šæœ¬å°è¯•äº† HTTP è¿æ¥")
        print("=" * 60)
    
    raise last_error if last_error else Exception("æ— æ³•è¿æ¥åˆ° Elasticsearch")


# ====== æ­¥éª¤ 5ï¼šåˆ›å»ºæ”¯æŒå‘é‡çš„ç´¢å¼• ======
def create_vector_index(es: Elasticsearch, index_name: str):
    """
    åˆ›å»ºæ”¯æŒå‘é‡æœç´¢çš„ Elasticsearch ç´¢å¼•
    
    å‚æ•°ï¼š
    - es: Elasticsearch å®¢æˆ·ç«¯å¯¹è±¡
    - index_name: ç´¢å¼•åç§°
    
    åŠŸèƒ½ï¼š
    - å®šä¹‰ç´¢å¼•æ˜ å°„ï¼ŒåŒ…å« dense_vector å­—æ®µç”¨äºå‘é‡æœç´¢
    - åŒæ—¶ä¿ç•™æ–‡æœ¬å­—æ®µç”¨äºæ··åˆæœç´¢
    - åˆ›å»ºç´¢å¼•ï¼ˆå¦‚æœå·²å­˜åœ¨åˆ™è·³è¿‡ï¼‰
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 2ï¼šåˆ›å»ºæ”¯æŒå‘é‡æœç´¢çš„ç´¢å¼•")
    print("=" * 60)
    
    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
    if es.indices.exists(index=index_name):
        print(f"âš ï¸  ç´¢å¼• '{index_name}' å·²å­˜åœ¨ï¼Œå°†ä½¿ç”¨ç°æœ‰ç´¢å¼•")
        print("   å¦‚éœ€é‡æ–°åˆ›å»ºï¼Œè¯·å…ˆåˆ é™¤ç°æœ‰ç´¢å¼•")
        return
    
    # å®šä¹‰ç´¢å¼•æ˜ å°„ï¼ŒåŒ…å«å‘é‡å­—æ®µ
    index_mapping = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
        },
        "mappings": {
            "properties": {
                # æ–‡æ¡£æ ‡é¢˜å­—æ®µï¼ˆæ–‡æœ¬æœç´¢ï¼‰
                "title": {
                    "type": "text",
                    "analyzer": "ik_max_word",  # ä½¿ç”¨ IK åˆ†è¯å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    "search_analyzer": "ik_max_word",
                    "fields": {
                        "keyword": {
                            "type": "keyword"
                        }
                    }
                },
                # æ–‡æ¡£æºæ–‡ä»¶è·¯å¾„
                "source": {
                    "type": "keyword"
                },
                # æ–‡æ¡£å—å†…å®¹ï¼ˆæ–‡æœ¬æœç´¢ï¼‰
                "content": {
                    "type": "text",
                    "analyzer": "ik_max_word",
                    "search_analyzer": "ik_max_word"
                },
                # å‘é‡å­—æ®µï¼ˆç”¨äºå‘é‡æœç´¢ï¼‰
                "content_vector": {
                    "type": "dense_vector",  # å¯†é›†å‘é‡ç±»å‹
                    "dims": EMBEDDING_DIMENSIONS,  # å‘é‡ç»´åº¦ï¼š1024
                    "index": True,  # å¯ç”¨å‘é‡ç´¢å¼•
                    "similarity": "cosine"  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
                },
                # å— ID
                "chunk_id": {
                    "type": "integer"
                },
                # é¡µç 
                "page_num": {
                    "type": "integer"
                },
                # Token æ•°é‡
                "token_count": {
                    "type": "integer"
                },
                # æ–‡æ¡£ç±»å‹
                "file_type": {
                    "type": "keyword"
                }
            }
        }
    }
    
    try:
        # åˆ›å»ºç´¢å¼•
        es.indices.create(
            index=index_name,
            settings=index_mapping["settings"],
            mappings=index_mapping["mappings"]
        )
        print(f"âœ… æˆåŠŸåˆ›å»ºå‘é‡ç´¢å¼•: {index_name}")
        print(f"   å‘é‡ç»´åº¦: {EMBEDDING_DIMENSIONS}")
        print(f"   ç›¸ä¼¼åº¦ç®—æ³•: cosineï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰")
    except Exception as e:
        error_msg = str(e)
        # å¦‚æœ IK åˆ†è¯å™¨ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨
        if "ik_max_word" in error_msg.lower():
            print(f"   âš ï¸  IK åˆ†è¯å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨")
            # ä¿®æ”¹æ˜ å°„ï¼Œä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨
            index_mapping["mappings"]["properties"]["title"]["analyzer"] = "standard"
            index_mapping["mappings"]["properties"]["title"]["search_analyzer"] = "standard"
            index_mapping["mappings"]["properties"]["content"]["analyzer"] = "standard"
            index_mapping["mappings"]["properties"]["content"]["search_analyzer"] = "standard"
            
            es.indices.create(
                index=index_name,
                settings=index_mapping["settings"],
                mappings=index_mapping["mappings"]
            )
            print(f"âœ… æˆåŠŸåˆ›å»ºå‘é‡ç´¢å¼•: {index_name}ï¼ˆä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨ï¼‰")
        else:
            print(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {str(e)}")
            raise


# ====== æ­¥éª¤ 6ï¼šè§£ææ–‡æ¡£å¹¶ç”Ÿæˆå‘é‡ ======
def parse_and_embed_documents(docs_folder: str, embedding_client: OpenAI) -> List[Dict]:
    """
    è§£ææ–‡æ¡£å¹¶ç”Ÿæˆå‘é‡åµŒå…¥
    
    å‚æ•°ï¼š
    - docs_folder: æ–‡æ¡£æ–‡ä»¶å¤¹è·¯å¾„
    - embedding_client: Embedding å®¢æˆ·ç«¯å¯¹è±¡
    
    è¿”å›ï¼š
    - æ–‡æ¡£å—åˆ—è¡¨ï¼Œæ¯ä¸ªå—åŒ…å«æ–‡æœ¬å†…å®¹å’Œå‘é‡åµŒå…¥
    
    åŠŸèƒ½ï¼š
    - è§£ææ–‡æ¡£å¹¶åˆ†å—
    - ä¸ºæ¯ä¸ªæ–‡æ¡£å—ç”Ÿæˆå‘é‡åµŒå…¥
    - è¿”å›åŒ…å«å‘é‡çš„æ–‡æ¡£å—
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 3ï¼šè§£ææ–‡æ¡£å¹¶ç”Ÿæˆå‘é‡åµŒå…¥")
    print("=" * 60)
    
    # åˆ›å»º DocParser å®ä¾‹
    doc_parser = DocParser({
        'max_ref_token': 20000,
        'parser_page_size': 500
    })
    
    # è·å–æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
    if not os.path.exists(docs_folder):
        raise FileNotFoundError(f"æ–‡æ¡£æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {docs_folder}")
    
    files = []
    for file in os.listdir(docs_folder):
        file_path = os.path.join(docs_folder, file)
        if os.path.isfile(file_path):
            files.append(file_path)
    
    print(f"ğŸ“ æ‰¾åˆ° {len(files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
    print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {EMBEDDING_MODEL}")
    print(f"ğŸ“ å‘é‡ç»´åº¦: {EMBEDDING_DIMENSIONS}\n")
    
    # è§£ææ‰€æœ‰æ–‡æ¡£å¹¶ç”Ÿæˆå‘é‡
    all_chunks = []
    for i, file_path in enumerate(files, 1):
        print(f"[{i}/{len(files)}] æ­£åœ¨å¤„ç†: {os.path.basename(file_path)}")
        
        try:
            # è§£ææ–‡æ¡£
            record = doc_parser.call(params={'url': file_path})
            file_ext = os.path.splitext(file_path)[1].lower().lstrip('.')
            
            # å¤„ç†æ¯ä¸ªæ–‡æ¡£å—
            for j, chunk in enumerate(record['raw'], 1):
                chunk_text = chunk['content']
                
                # ç”Ÿæˆå‘é‡åµŒå…¥
                print(f"   ç”Ÿæˆå‘é‡ [{j}/{len(record['raw'])}]...", end='', flush=True)
                try:
                    embedding = generate_embedding(chunk_text, embedding_client)
                    print(" âœ…")
                except Exception as e:
                    print(f" âŒ å¤±è´¥: {str(e)}")
                    continue
                
                # æ„å»ºæ–‡æ¡£å—æ•°æ®
                chunk_data = {
                    'title': record['title'],
                    'source': file_path,
                    'content': chunk_text,
                    'content_vector': embedding,  # æ·»åŠ å‘é‡å­—æ®µ
                    'chunk_id': chunk['metadata'].get('chunk_id', 0),
                    'page_num': chunk['metadata'].get('page_num', 1),
                    'token_count': chunk['token'],
                    'file_type': file_ext
                }
                all_chunks.append(chunk_data)
            
            print(f"   âœ… å®Œæˆï¼Œç”Ÿæˆ {len(record['raw'])} ä¸ªå‘é‡å—\n")
            
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {str(e)}\n")
            continue
    
    print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆï¼")
    print(f"   æ€»å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªå¸¦å‘é‡çš„æ–‡æ¡£å—")
    
    return all_chunks


# ====== æ­¥éª¤ 7ï¼šç´¢å¼•æ–‡æ¡£åˆ° Elasticsearch ======
def index_documents_with_vectors(es: Elasticsearch, index_name: str, chunks: List[Dict]):
    """
    å°†æ–‡æ¡£å—å’Œå‘é‡ç´¢å¼•åˆ° Elasticsearch
    
    å‚æ•°ï¼š
    - es: Elasticsearch å®¢æˆ·ç«¯å¯¹è±¡
    - index_name: ç´¢å¼•åç§°
    - chunks: åŒ…å«å‘é‡çš„æ–‡æ¡£å—åˆ—è¡¨
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 4ï¼šç´¢å¼•æ–‡æ¡£å’Œå‘é‡åˆ° Elasticsearch")
    print("=" * 60)
    
    if not chunks:
        print("âš ï¸  æ²¡æœ‰æ–‡æ¡£å—éœ€è¦ç´¢å¼•")
        return
    
    # å‡†å¤‡æ‰¹é‡ç´¢å¼•çš„æ•°æ®
    actions = []
    for chunk in chunks:
        # ç”Ÿæˆæ–‡æ¡£ ID
        doc_id = f"{hashlib.sha256(chunk['source'].encode()).hexdigest()}_{chunk['chunk_id']}"
        
        # æ„å»ºè¦ç´¢å¼•çš„æ–‡æ¡£ï¼ˆåŒ…å«å‘é‡ï¼‰
        action = {
            "_index": index_name,
            "_id": doc_id,
            "_source": chunk
        }
        actions.append(action)
    
    print(f"ğŸ“¤ å‡†å¤‡ç´¢å¼• {len(actions)} ä¸ªæ–‡æ¡£å—ï¼ˆåŒ…å«å‘é‡ï¼‰...")
    
    try:
        # æ‰¹é‡ç´¢å¼•
        success_count, failed_items = bulk(es, actions, chunk_size=50, request_timeout=120)
        
        print(f"âœ… ç´¢å¼•å®Œæˆï¼")
        print(f"   æˆåŠŸç´¢å¼•: {success_count} ä¸ªæ–‡æ¡£")
        if failed_items:
            print(f"   å¤±è´¥: {len(failed_items)} ä¸ªæ–‡æ¡£")
        
        # åˆ·æ–°ç´¢å¼•
        es.indices.refresh(index=index_name)
        print(f"   ç´¢å¼•å·²åˆ·æ–°ï¼Œæ–‡æ¡£å¯ç«‹å³æœç´¢")
        
    except Exception as e:
        print(f"âŒ ç´¢å¼•å¤±è´¥: {str(e)}")
        raise


# ====== æ­¥éª¤ 8ï¼šå‘é‡æœç´¢ ======
def vector_search(es: Elasticsearch, index_name: str, search_query: str, top_k: int = 10, 
                  use_hybrid: bool = True):
    """
    ä½¿ç”¨å‘é‡æœç´¢åœ¨ Elasticsearch ä¸­æ£€ç´¢æ–‡æ¡£
    
    å‚æ•°ï¼š
    - es: Elasticsearch å®¢æˆ·ç«¯å¯¹è±¡
    - index_name: ç´¢å¼•åç§°
    - search_query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
    - top_k: è¿”å›å‰ K ä¸ªç»“æœ
    - use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæœç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
    
    è¿”å›ï¼š
    - æœç´¢ç»“æœåˆ—è¡¨
    
    åŠŸèƒ½ï¼š
    - å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
    - ä½¿ç”¨ knn æŸ¥è¯¢è¿›è¡Œå‘é‡æœç´¢
    - å¯é€‰ï¼šç»“åˆå…³é”®è¯æœç´¢ï¼ˆæ··åˆæœç´¢ï¼‰
    """
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 5ï¼šæ‰§è¡Œå‘é‡æœç´¢")
    print("=" * 60)
    
    print(f"ğŸ” æœç´¢æŸ¥è¯¢: {search_query}")
    print(f"   è¿”å›å‰ {top_k} ä¸ªç»“æœ")
    print(f"   æœç´¢æ¨¡å¼: {'æ··åˆæœç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰' if use_hybrid else 'çº¯å‘é‡æœç´¢'}")
    
    # åˆå§‹åŒ– Embedding å®¢æˆ·ç«¯
    embedding_client = init_embedding_client()
    
    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    print(f"\nğŸ“Š æ­£åœ¨ç”ŸæˆæŸ¥è¯¢å‘é‡...")
    try:
        query_vector = generate_embedding(search_query, embedding_client)
        print(f"   âœ… æŸ¥è¯¢å‘é‡ç”Ÿæˆå®Œæˆï¼ˆç»´åº¦: {len(query_vector)}ï¼‰")
    except Exception as e:
        print(f"   âŒ ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥: {str(e)}")
        raise
    
    # æ„å»ºæœç´¢æŸ¥è¯¢
    if use_hybrid:
        # æ··åˆæœç´¢ï¼šç»“åˆå‘é‡æœç´¢å’Œå…³é”®è¯æœç´¢
        search_body = {
            "knn": {
                "field": "content_vector",  # å‘é‡å­—æ®µ
                "query_vector": query_vector,  # æŸ¥è¯¢å‘é‡
                "k": top_k,  # è¿”å›çš„æœ€è¿‘é‚»æ•°é‡
                "num_candidates": top_k * 10  # å€™é€‰æ•°é‡ï¼ˆè¶Šå¤§è¶Šå‡†ç¡®ï¼Œä½†è¶Šæ…¢ï¼‰
            },
            "query": {
                "multi_match": {
                    "query": search_query,
                    "fields": ["title^2", "content"],
                    "type": "best_fields"
                }
            },
            "size": top_k,
            "_source": {
                "excludes": ["content_vector"]  # ä¸è¿”å›å‘é‡å­—æ®µï¼ˆå‡å°‘ä¼ è¾“é‡ï¼‰
            }
        }
    else:
        # çº¯å‘é‡æœç´¢
        search_body = {
            "knn": {
                "field": "content_vector",
                "query_vector": query_vector,
                "k": top_k,
                "num_candidates": top_k * 10
            },
            "size": top_k,
            "_source": {
                "excludes": ["content_vector"]
            }
        }
    
    try:
        # æ‰§è¡Œæœç´¢
        response = es.search(index=index_name, **search_body)
        
        # è§£ææœç´¢ç»“æœ
        hits = response['hits']['hits']
        total = response['hits']['total']['value']
        
        print(f"\nâœ… æœç´¢å®Œæˆï¼")
        print(f"   æ‰¾åˆ° {total} ä¸ªç›¸å…³æ–‡æ¡£")
        print(f"   è¿”å›å‰ {len(hits)} ä¸ªç»“æœ\n")
        
        return hits
        
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
        raise


# ====== æ­¥éª¤ 9ï¼šæ˜¾ç¤ºæœç´¢ç»“æœ ======
def display_search_results(hits: List[Dict]):
    """
    æ ¼å¼åŒ–å¹¶æ˜¾ç¤ºæœç´¢ç»“æœ
    
    å‚æ•°ï¼š
    - hits: æœç´¢ç»“æœåˆ—è¡¨
    """
    print("=" * 60)
    print("æ­¥éª¤ 6ï¼šæ˜¾ç¤ºæœç´¢ç»“æœ")
    print("=" * 60)
    
    if not hits:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        return
    
    # éå†æ˜¾ç¤ºæ¯ä¸ªæœç´¢ç»“æœ
    for i, hit in enumerate(hits, 1):
        score = hit['_score']  # ç›¸å…³æ€§è¯„åˆ†ï¼ˆå‘é‡ç›¸ä¼¼åº¦æˆ–æ··åˆè¯„åˆ†ï¼‰
        source = hit['_source']
        
        print(f"\n{'=' * 60}")
        print(f"ç»“æœ {i} (ç›¸å…³æ€§è¯„åˆ†: {score:.4f})")
        print(f"{'=' * 60}")
        
        # æ˜¾ç¤ºæ ‡é¢˜
        print(f"ğŸ“„ æ ‡é¢˜: {source.get('title', 'æ— æ ‡é¢˜')}")
        
        # æ˜¾ç¤ºæ¥æºæ–‡ä»¶
        source_file = os.path.basename(source.get('source', 'æœªçŸ¥'))
        print(f"ğŸ“ æ¥æº: {source_file}")
        
        # æ˜¾ç¤ºæ–‡ä»¶ç±»å‹å’Œå—ä¿¡æ¯
        print(f"ğŸ“Š ä¿¡æ¯: æ–‡ä»¶ç±»å‹={source.get('file_type', 'æœªçŸ¥')}, "
              f"å—ID={source.get('chunk_id', 0)}, "
              f"Tokenæ•°={source.get('token_count', 0)}")
        
        # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
        content = source.get('content', '')
        preview = content[:300] + ('...' if len(content) > 300 else '')
        print(f"\nğŸ’¡ å†…å®¹é¢„è§ˆ:")
        print(f"   {preview}")
        
        print()
    
    print("=" * 60)


# ====== ä¸»ç¨‹åº ======
def main():
    """
    ä¸»ç¨‹åºï¼šæ‰§è¡Œå®Œæ•´çš„å‘é‡ç´¢å¼•å’Œæœç´¢æµç¨‹
    
    æµç¨‹ï¼š
    1. è¿æ¥åˆ° Elasticsearch
    2. åˆ›å»ºæ”¯æŒå‘é‡çš„ç´¢å¼•
    3. è§£ææ–‡æ¡£å¹¶ç”Ÿæˆå‘é‡
    4. ç´¢å¼•æ–‡æ¡£å’Œå‘é‡åˆ° Elasticsearch
    5. æ‰§è¡Œå‘é‡æœç´¢
    6. æ˜¾ç¤ºæœç´¢ç»“æœ
    """
    print("\n" + "ğŸš€" * 30)
    print("åŸºäº Embedding å‘é‡çš„ Elasticsearch æ–‡æ¡£æœç´¢ç³»ç»Ÿ")
    print("ğŸš€" * 30 + "\n")
    
    try:
        # æ­¥éª¤ 1ï¼šåˆå§‹åŒ– Embedding å®¢æˆ·ç«¯
        print("=" * 60)
        print("åˆå§‹åŒ– Embedding å®¢æˆ·ç«¯")
        print("=" * 60)
        embedding_client = init_embedding_client()
        print(f"âœ… Embedding å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        print(f"   æ¨¡å‹: {EMBEDDING_MODEL}")
        print(f"   ç»´åº¦: {EMBEDDING_DIMENSIONS}\n")
        
        # æ­¥éª¤ 2ï¼šè¿æ¥åˆ° Elasticsearch
        es = connect_elasticsearch()
        
        # æ­¥éª¤ 3ï¼šåˆ›å»ºå‘é‡ç´¢å¼•
        create_vector_index(es, INDEX_NAME)
        
        # æ­¥éª¤ 4ï¼šè§£ææ–‡æ¡£å¹¶ç”Ÿæˆå‘é‡
        chunks = parse_and_embed_documents(DOCS_FOLDER, embedding_client)
        
        # æ­¥éª¤ 5ï¼šç´¢å¼•æ–‡æ¡£å’Œå‘é‡
        if chunks:
            index_documents_with_vectors(es, INDEX_NAME, chunks)
        else:
            print("âš ï¸  æ²¡æœ‰æ–‡æ¡£å—éœ€è¦ç´¢å¼•ï¼Œè·³è¿‡ç´¢å¼•æ­¥éª¤")
        
        # æ­¥éª¤ 6ï¼šæ‰§è¡Œå‘é‡æœç´¢
        search_query = "å·¥ä¼¤ä¿é™©å’Œé›‡ä¸»é™©æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
        hits = vector_search(es, INDEX_NAME, search_query, top_k=10, use_hybrid=True)
        
        # æ­¥éª¤ 7ï¼šæ˜¾ç¤ºæœç´¢ç»“æœ
        display_search_results(hits)
        
        print("\n" + "âœ…" * 30)
        print("æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆï¼")
        print("âœ…" * 30 + "\n")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


# ====== ç¨‹åºå…¥å£ ======
if __name__ == '__main__':
    exit(main())

