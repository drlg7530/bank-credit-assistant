"""
RAGæŸ¥è¯¢æ¨¡å—
åŠŸèƒ½ï¼š
1. Queryæ”¹å†™ï¼ˆè°ƒç”¨å¤§æ¨¡å‹ï¼‰
2. å‘é‡åŒ–æŸ¥è¯¢
3. Elasticsearchå‘é‡æœç´¢
4. é‡æ’åºï¼ˆå¯é…ç½®ï¼‰
5. å¤§æ¨¡å‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
"""

import os
import sys
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Generator
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼ˆä»src/rag/åˆ°é¡¹ç›®æ ¹ç›®å½•éœ€è¦ä¸¤çº§ï¼‰
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é…ç½®
from config.elasticsearch import ES_CONFIG, INDEX_CONFIG
from config.prompts import get_rag_query_prompt, TODAY
from config.rag_config import RAG_CONFIG, get_rag_config, is_rerank_enabled, is_rewrite_enabled

# å¯¼å…¥ç›‘æ§æ¨¡å—
from src.utils.llm_monitor import set_token_info
from src.utils.monitor import extract_token_info_from_response

# Embeddingæ¨¡å‹é…ç½®ï¼ˆä¸build_vector_db.pyä¿æŒä¸€è‡´ï¼‰
EMBEDDING_MODEL_NAME = os.getenv('EMBEDDING_MODEL', 'Qwen/Qwen3-Embedding-0.6B')

# å¤§æ¨¡å‹é…ç½®ï¼ˆä¸generate_qa_pairs.pyä¿æŒä¸€è‡´ï¼‰
LLM_MODE = os.getenv('LLM_MODE', 'bailian').lower()
DASHSCOPE_API_KEY = os.getenv('DASHSCOPE_API_KEY', '')
BAILIAN_MODEL = os.getenv('BAILIAN_MODEL', 'qwen-plus')
LOCAL_MODEL_PATH = os.getenv('LOCAL_MODEL_PATH', 'Qwen/Qwen2.5-7B-Instruct')

# ============================================================================
# ä¾èµ–æ£€æŸ¥
# ============================================================================

try:
    from elasticsearch import Elasticsearch
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False
    print("âš  è­¦å‘Š: elasticsearchæœªå®‰è£…")

try:
    from modelscope import snapshot_download
    from transformers import AutoTokenizer, AutoModel
    import torch
    MODELSCOPE_AVAILABLE = True
except ImportError:
    MODELSCOPE_AVAILABLE = False
    print("âš  è­¦å‘Š: modelscopeæˆ–transformersæœªå®‰è£…")

try:
    from dashscope import Generation
    import dashscope
    DASHSCOPE_AVAILABLE = True
except ImportError:
    DASHSCOPE_AVAILABLE = False

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

# ============================================================================
# å…¨å±€å˜é‡ï¼ˆæ¨¡å‹ç¼“å­˜ï¼‰
# ============================================================================

_embedding_model = None
_embedding_tokenizer = None
_local_llm_model = None
_local_llm_tokenizer = None

# ============================================================================
# æ•°æ®ç±»å®šä¹‰
# ============================================================================

@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç±»"""
    content: str              # æ–‡æ¡£å†…å®¹
    score: float             # ç›¸ä¼¼åº¦åˆ†æ•°
    metadata: Dict           # å…ƒæ•°æ®
    chunk_id: str            # æ–‡æœ¬å—ID

# ============================================================================
# Embeddingæ¨¡å‹åŠ è½½å’Œå‘é‡åŒ–
# ============================================================================

def load_embedding_model(model_path: str = None):
    """
    åŠ è½½Embeddingæ¨¡å‹ï¼ˆå…¨å±€ç¼“å­˜ï¼‰
    
    å‚æ•°:
        model_path: æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneï¼Œä½¿ç”¨å…¨å±€é…ç½®
    """
    global _embedding_model, _embedding_tokenizer
    
    if _embedding_model is not None:
        return _embedding_model, _embedding_tokenizer
    
    if not MODELSCOPE_AVAILABLE:
        raise Exception("modelscopeæˆ–transformersæœªå®‰è£…")
    
    if model_path is None:
        model_path = EMBEDDING_MODEL_NAME
    
    print(f"  æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹: {model_path}")
    
    try:
        # ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœæœªä¸‹è½½ï¼‰
        model_dir = snapshot_download(model_path, cache_dir='./models/embedding')
        
        # åŠ è½½tokenizerå’Œmodel
        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰GPU
        use_cpu = os.getenv('FORCE_CPU', 'false').lower() == 'true'
        if use_cpu:
            device_map = 'cpu'
        elif torch.cuda.is_available():
            device_map = 'auto'
        else:
            device_map = 'cpu'
        
        model = AutoModel.from_pretrained(model_dir, trust_remote_code=True, device_map=device_map)
        model.eval()
        
        _embedding_model = model
        _embedding_tokenizer = tokenizer
        
        print(f"  âœ“ Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, tokenizer
        
    except Exception as e:
        raise Exception(f"Embeddingæ¨¡å‹åŠ è½½å¤±è´¥: {e}")


def generate_query_embedding(query: str) -> np.ndarray:
    """
    ç”ŸæˆæŸ¥è¯¢æ–‡æœ¬çš„embeddingå‘é‡
    
    å‚æ•°:
        query: æŸ¥è¯¢æ–‡æœ¬
    
    è¿”å›:
        np.ndarray: embeddingå‘é‡
    """
    model, tokenizer = load_embedding_model()
    
    # Tokenize
    inputs = tokenizer(
        query,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors='pt'
    )
    
    # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # ç”Ÿæˆembedding
    with torch.no_grad():
        outputs = model(**inputs)
        # æå–embeddingï¼ˆä¸build_vector_db.pyä¿æŒä¸€è‡´ï¼‰
        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
            embedding = outputs.pooler_output.cpu().numpy()[0]
        elif hasattr(outputs, 'last_hidden_state'):
            embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()[0]
        else:
            embedding = outputs.cpu().numpy()[0] if isinstance(outputs, torch.Tensor) else outputs[0]
            if isinstance(embedding, torch.Tensor):
                embedding = embedding.numpy()
    
    return embedding

# ============================================================================
# å¤§æ¨¡å‹è°ƒç”¨ï¼ˆå¤ç”¨generate_qa_pairs.pyçš„é€»è¾‘ï¼‰
# ============================================================================

def load_local_llm_model(model_path: str = None):
    """åŠ è½½æœ¬åœ°LLMæ¨¡å‹ï¼ˆå…¨å±€ç¼“å­˜ï¼‰"""
    global _local_llm_model, _local_llm_tokenizer
    
    if _local_llm_model is not None:
        return _local_llm_model, _local_llm_tokenizer
    
    if not TRANSFORMERS_AVAILABLE:
        raise Exception("transformersæœªå®‰è£…")
    
    if model_path is None:
        model_path = LOCAL_MODEL_PATH
    
    print(f"  æ­£åœ¨åŠ è½½æœ¬åœ°LLMæ¨¡å‹: {model_path}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        use_cpu = os.getenv('FORCE_CPU', 'false').lower() == 'true'
        if use_cpu:
            device_map = 'cpu'
        elif torch.cuda.is_available():
            device_map = 'auto'
        else:
            device_map = 'cpu'
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            device_map=device_map,
            torch_dtype=torch.float16 if device_map != 'cpu' else torch.float32
        )
        model.eval()
        
        _local_llm_model = model
        _local_llm_tokenizer = tokenizer
        
        print(f"  âœ“ æœ¬åœ°LLMæ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, tokenizer
        
    except Exception as e:
        raise Exception(f"æœ¬åœ°LLMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")


def call_local_llm(prompt: str, max_length: int = 1000, module: str = "rag_query") -> str:
    """
    è°ƒç”¨æœ¬åœ°LLMæ¨¡å‹
    
    å‚æ•°:
        prompt: æç¤ºè¯
        max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
        module: æ¨¡å—åç§°ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç ï¼‰
    
    è¿”å›:
        str: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
    """
    model, tokenizer = load_local_llm_model()
    
    # æ„å»ºæç¤ºè¯ï¼ˆQwen2.5æ ¼å¼ï¼‰
    formatted_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    # Tokenizeï¼ˆè·å–å®é™…tokenæ•°ï¼‰
    inputs = tokenizer.encode(formatted_prompt, return_tensors='pt')
    prompt_tokens = inputs.shape[1]  # è·å–å®é™…tokenæ•°
    
    device = next(model.parameters()).device
    inputs = inputs.to(device)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=inputs.shape[1] + max_length,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç 
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå–assistantçš„å›å¤
    if '<|im_start|>assistant\n' in generated_text:
        response = generated_text.split('<|im_start|>assistant\n')[-1]
        response = response.split('<|im_end|>')[0].strip()
    else:
        response = generated_text[len(formatted_prompt):].strip()
    
    # è®¡ç®—completion tokensï¼ˆç”Ÿæˆçš„æ€»tokenæ•°å‡å»è¾“å…¥çš„tokenæ•°ï¼‰
    # outputs.shape[1] æ˜¯å®Œæ•´åºåˆ—é•¿åº¦ï¼ˆè¾“å…¥+ç”Ÿæˆï¼‰ï¼Œprompt_tokens æ˜¯è¾“å…¥é•¿åº¦
    # å·®å€¼å°±æ˜¯ç”Ÿæˆçš„tokenæ•°ï¼ˆä½¿ç”¨max(0, ...)é˜²æ­¢è´Ÿæ•°ï¼‰
    completion_tokens = max(0, outputs.shape[1] - prompt_tokens)
    
    # è®¾ç½®tokenä¿¡æ¯åˆ°çº¿ç¨‹æœ¬åœ°å­˜å‚¨ï¼ˆä¾›ç›‘æ§è£…é¥°å™¨ä½¿ç”¨ï¼‰
    set_token_info(prompt_tokens, completion_tokens)
    
    return response


def call_bailian_api(prompt: str, module: str = "rag_query") -> str:
    """
    è°ƒç”¨ç™¾ç‚¼API
    
    å‚æ•°:
        prompt: æç¤ºè¯
        module: æ¨¡å—åç§°ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç ï¼‰
    
    è¿”å›:
        str: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
    """
    if not DASHSCOPE_AVAILABLE:
        raise Exception("dashscopeæœªå®‰è£…")
    
    if not DASHSCOPE_API_KEY:
        raise ValueError("ç™¾ç‚¼APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
    
    dashscope.api_key = DASHSCOPE_API_KEY
    
    response = Generation.call(
        model=BAILIAN_MODEL,
        prompt=prompt,
        temperature=0.7,
        max_tokens=1000,
        result_format='message'
    )
    
    if response.status_code == 200:
        # æå–tokenä¿¡æ¯ï¼ˆä»å“åº”å¯¹è±¡ä¸­ï¼‰
        prompt_tokens = 0
        completion_tokens = 0
        try:
            token_info = extract_token_info_from_response(response, BAILIAN_MODEL)
            prompt_tokens = token_info.get('prompt_tokens', 0)
            completion_tokens = token_info.get('completion_tokens', 0)
        except Exception:
            # å¦‚æœæå–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼0
            pass
        
        content = None
        if 'output' in response:
            output = response['output']
            if 'choices' in output and len(output['choices']) > 0:
                message = output['choices'][0].get('message', {})
                if 'content' in message:
                    content = message['content'].strip()
            if not content and 'text' in output:
                content = output['text'].strip()
        
        if not content and 'text' in response:
            content = response['text'].strip()
        if not content and 'content' in response:
            content = response['content'].strip()
        
        if content:
            # è®¾ç½®tokenä¿¡æ¯åˆ°çº¿ç¨‹æœ¬åœ°å­˜å‚¨ï¼ˆä¾›ç›‘æ§è£…é¥°å™¨ä½¿ç”¨ï¼‰
            set_token_info(prompt_tokens, completion_tokens)
            return content
    
    error_msg = f"ç™¾ç‚¼APIè°ƒç”¨å¤±è´¥: {response.status_code}"
    if hasattr(response, 'message'):
        error_msg += f" - {response.message}"
    raise Exception(error_msg)


def call_bailian_api_stream(prompt: str, module: str = "rag_query") -> Generator[str, None, None]:
    """
    è°ƒç”¨ç™¾ç‚¼APIï¼ˆæµå¼è¾“å‡ºï¼‰
    
    å‚æ•°:
        prompt: æç¤ºè¯
        module: æ¨¡å—åç§°ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç ï¼‰
    
    è¿”å›:
        Generator[str, None, None]: ç”Ÿæˆå™¨ï¼Œé€æ­¥è¿”å›æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
    """
    if not DASHSCOPE_AVAILABLE:
        raise Exception("dashscopeæœªå®‰è£…")
    
    if not DASHSCOPE_API_KEY:
        raise ValueError("ç™¾ç‚¼APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡")
    
    dashscope.api_key = DASHSCOPE_API_KEY
    
    # ä½¿ç”¨æµå¼è°ƒç”¨
    responses = Generation.call(
        model=BAILIAN_MODEL,
        prompt=prompt,
        temperature=0.7,
        max_tokens=1000,
        result_format='message',
        stream=True  # å¯ç”¨æµå¼è¾“å‡º
    )
    
    full_content = ""
    prompt_tokens = 0
    completion_tokens = 0
    
    try:
        chunk_count = 0
        previous_content = ""  # ç”¨äºè®¡ç®—å¢é‡å†…å®¹
        
        for response in responses:
            if response.status_code == 200:
                # æå–å†…å®¹
                # ç™¾ç‚¼APIæµå¼å“åº”æ ¼å¼ï¼š
                # - delta.content: å¢é‡å†…å®¹ï¼ˆæ¨èä½¿ç”¨ï¼‰
                # - message.content: ç´¯ç§¯çš„å®Œæ•´å†…å®¹ï¼ˆéœ€è¦è®¡ç®—å¢é‡ï¼‰
                if 'output' in response:
                    output = response['output']
                    if 'choices' in output and len(output['choices']) > 0:
                        choice = output['choices'][0]
                        delta = choice.get('delta', {})
                        message = choice.get('message', {})
                        
                        # ä¼˜å…ˆä»deltaè·å–å¢é‡å†…å®¹
                        chunk_text = None
                        if 'content' in delta:
                            # delta.content æ˜¯å¢é‡å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨
                            chunk_text = delta['content']
                        elif 'content' in message:
                            # message.content æ˜¯ç´¯ç§¯å†…å®¹ï¼Œéœ€è¦è®¡ç®—å¢é‡
                            current_content = message['content']
                            if current_content.startswith(previous_content):
                                # è®¡ç®—å¢é‡éƒ¨åˆ†
                                chunk_text = current_content[len(previous_content):]
                                previous_content = current_content
                            else:
                                # å¦‚æœå†…å®¹ä¸è¿ç»­ï¼Œå¯èƒ½æ˜¯æ–°çš„å“åº”ï¼Œç›´æ¥ä½¿ç”¨
                                chunk_text = current_content
                                previous_content = current_content
                        
                        if chunk_text:
                            full_content += chunk_text
                            chunk_count += 1
                            if chunk_count <= 3:  # åªæ‰“å°å‰3ä¸ªchunkçš„æ—¥å¿—
                                print(f"[call_bailian_api_stream] æ”¶åˆ°chunk #{chunk_count} (é•¿åº¦: {len(chunk_text)}): {chunk_text[:50]}...")
                            yield chunk_text
                        else:
                            # æ£€æŸ¥æ˜¯å¦æœ‰finish_reasonï¼ˆè¡¨ç¤ºæµå¼ç»“æŸï¼‰
                            finish_reason = choice.get('finish_reason')
                            if finish_reason:
                                print(f"[call_bailian_api_stream] æµå¼ç»“æŸï¼Œfinish_reason: {finish_reason}")
                else:
                    # æ£€æŸ¥å“åº”ç»“æ„
                    if chunk_count == 0:
                        print(f"[call_bailian_api_stream] å“åº”ä¸­æ²¡æœ‰outputå­—æ®µï¼Œå“åº”ç»“æ„: {list(response.keys())}")
                
                # æå–tokenä¿¡æ¯ï¼ˆä»æœ€åä¸€ä¸ªå“åº”ä¸­ï¼‰
                try:
                    token_info = extract_token_info_from_response(response, BAILIAN_MODEL)
                    prompt_tokens = token_info.get('prompt_tokens', 0)
                    completion_tokens = token_info.get('completion_tokens', 0)
                except Exception:
                    pass
            else:
                error_msg = f"ç™¾ç‚¼APIæµå¼è°ƒç”¨å¤±è´¥: {response.status_code}"
                if hasattr(response, 'message'):
                    error_msg += f" - {response.message}"
                print(f"[call_bailian_api_stream] APIé”™è¯¯: {error_msg}")
                raise Exception(error_msg)
        
        print(f"[call_bailian_api_stream] æµå¼è°ƒç”¨å®Œæˆï¼Œæ€»å…±æ”¶åˆ° {chunk_count} ä¸ªchunkï¼Œå®Œæ•´å†…å®¹é•¿åº¦: {len(full_content)}")
        
        # è®¾ç½®tokenä¿¡æ¯åˆ°çº¿ç¨‹æœ¬åœ°å­˜å‚¨ï¼ˆä¾›ç›‘æ§è£…é¥°å™¨ä½¿ç”¨ï¼‰
        set_token_info(prompt_tokens, completion_tokens)
        
    except Exception as e:
        # å¦‚æœæµå¼è°ƒç”¨å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
        error_msg = f"ç™¾ç‚¼APIæµå¼è°ƒç”¨å¼‚å¸¸: {e}"
        print(f"[call_bailian_api_stream] å¼‚å¸¸: {error_msg}")
        import traceback
        traceback.print_exc()
        raise Exception(error_msg)


def call_llm(prompt: str, mode: str = None, module: str = "rag_query", stream: bool = False) -> str:
    """
    è°ƒç”¨å¤§æ¨¡å‹ï¼ˆç»Ÿä¸€æ¥å£ï¼Œå¸¦ç›‘æ§ï¼‰
    
    å‚æ•°:
        prompt: æç¤ºè¯
        mode: è°ƒç”¨æ¨¡å¼ï¼ˆ'bailian' æˆ– 'local'ï¼‰ï¼Œå¦‚æœä¸ºNoneï¼Œä½¿ç”¨å…¨å±€é…ç½®
        module: æ¨¡å—åç§°ï¼ˆç”¨äºç›‘æ§ï¼‰
        stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆé»˜è®¤Falseï¼‰
    
    è¿”å›:
        str: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå¦‚æœstream=Trueï¼Œè¿”å›å®Œæ•´æ–‡æœ¬ï¼‰
    """
    if mode is None:
        mode = LLM_MODE
    
    if stream:
        # æµå¼è¾“å‡ºï¼ˆç›®å‰åªæ”¯æŒç™¾ç‚¼APIï¼‰
        if mode == 'local':
            # æœ¬åœ°æ¨¡å‹æš‚ä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œé™çº§ä¸ºæ™®é€šè°ƒç”¨
            return call_local_llm(prompt, module=module)
        else:
            # ç™¾ç‚¼APIæµå¼è¾“å‡ºï¼Œæ”¶é›†æ‰€æœ‰ç‰‡æ®µåè¿”å›å®Œæ•´æ–‡æœ¬
            full_text = ""
            for chunk in call_bailian_api_stream(prompt, module=module):
                full_text += chunk
            return full_text
    else:
        # æ™®é€šè¾“å‡º
        if mode == 'local':
            return call_local_llm(prompt, module=module)
        else:
            return call_bailian_api(prompt, module=module)


def call_llm_stream(prompt: str, mode: str = None, module: str = "rag_query") -> Generator[str, None, None]:
    """
    è°ƒç”¨å¤§æ¨¡å‹ï¼ˆæµå¼è¾“å‡ºï¼Œè¿”å›ç”Ÿæˆå™¨ï¼‰
    
    å‚æ•°:
        prompt: æç¤ºè¯
        mode: è°ƒç”¨æ¨¡å¼ï¼ˆ'bailian' æˆ– 'local'ï¼‰ï¼Œå¦‚æœä¸ºNoneï¼Œä½¿ç”¨å…¨å±€é…ç½®
        module: æ¨¡å—åç§°ï¼ˆç”¨äºç›‘æ§ï¼‰
    
    è¿”å›:
        Generator[str, None, None]: ç”Ÿæˆå™¨ï¼Œé€æ­¥è¿”å›æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
    """
    if mode is None:
        mode = LLM_MODE
    
    if mode == 'local':
        # æœ¬åœ°æ¨¡å‹æš‚ä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œé™çº§ä¸ºæ™®é€šè°ƒç”¨åé€å­—ç¬¦è¿”å›
        full_text = call_local_llm(prompt, module=module)
        # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼ˆé€å­—ç¬¦è¿”å›ï¼‰
        for char in full_text:
            yield char
    else:
        # ç™¾ç‚¼APIæµå¼è¾“å‡º
        yield from call_bailian_api_stream(prompt, module=module)

# ============================================================================
# Elasticsearchå‘é‡æœç´¢
# ============================================================================

def search_vectors(
    query_vector: np.ndarray,
    index_name: str,
    domain: str = None,
    role: str = 'å®¢æˆ·ç»ç†',
    top_k: int = None,
    filters: Dict = None
) -> List[SearchResult]:
    """
    åœ¨Elasticsearchä¸­æœç´¢ç›¸ä¼¼å‘é‡
    
    å‚æ•°:
        query_vector: æŸ¥è¯¢å‘é‡
        index_name: ç´¢å¼•åç§°
        domain: åŸŸç±»å‹ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
        role: ç”¨æˆ·è§’è‰²ï¼ˆç”¨äºæƒé™è¿‡æ»¤ï¼‰
        top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
        filters: é¢å¤–çš„è¿‡æ»¤æ¡ä»¶
    
    è¿”å›:
        List[SearchResult]: æœç´¢ç»“æœåˆ—è¡¨
    """
    if not ELASTICSEARCH_AVAILABLE:
        raise Exception("elasticsearchæœªå®‰è£…")
    
    if top_k is None:
        config = get_rag_config()
        top_k = config['top_k']
    
    # è¿æ¥Elasticsearch
    es_client = Elasticsearch(**ES_CONFIG)
    
    # æ„å»ºæŸ¥è¯¢
    query = {
        "knn": {
            "field": "embedding",
            "query_vector": query_vector.tolist(),
            "k": top_k,
            "num_candidates": max(50, top_k * 10),  # æ¨èå€¼ï¼šè‡³å°‘ 50ï¼Œæˆ– top_k*10
            # "num_candidates": top_k * 2,  # å€™é€‰æ•°é‡
        }
    }
    
    # æ·»åŠ è¿‡æ»¤æ¡ä»¶
    must_filters = []
    
    # æƒé™è¿‡æ»¤ï¼ˆæ ¹æ®ç”¨æˆ·è§’è‰²ï¼‰
    if role:
        must_filters.append({"term": {"role": role}})
    
    # åŸŸè¿‡æ»¤ï¼ˆä½¿ç”¨è‹±æ–‡å€¼ï¼špolicy/systemï¼‰
    if domain:
        must_filters.append({"term": {"domain": domain}})
    
    # çŠ¶æ€è¿‡æ»¤ï¼ˆåªæŸ¥è¯¢ç”Ÿæ•ˆçš„æ–‡æ¡£ï¼‰
    must_filters.append({"term": {"status": "ç”Ÿæ•ˆ"}})
    
    # æ·»åŠ é¢å¤–è¿‡æ»¤æ¡ä»¶
    if filters:
        for key, value in filters.items():
            must_filters.append({"term": {key: value}})
    
    # å¦‚æœæœ‰è¿‡æ»¤æ¡ä»¶ï¼Œæ·»åŠ åˆ°æŸ¥è¯¢ä¸­
    if must_filters:
        query["knn"]["filter"] = {
            "bool": {
                "must": must_filters
            }
        }
    
    # è°ƒè¯•ï¼šè¾“å‡ºæŸ¥è¯¢ä¿¡æ¯
    # print(f"  ğŸ” ç´¢å¼•åç§°: {index_name}")
    # print(f"  ğŸ” è¿‡æ»¤æ¡ä»¶: {must_filters}")
    # print(f"  ğŸ” æŸ¥è¯¢top_k: {top_k}")
    
    # æ‰§è¡Œæœç´¢
    try:
        # å…ˆæ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        if not es_client.indices.exists(index=index_name):
            print(f"  âš  ç´¢å¼• {index_name} ä¸å­˜åœ¨ï¼")
            return []
        
        # æ£€æŸ¥ç´¢å¼•ä¸­çš„æ–‡æ¡£æ€»æ•°
        index_stats = es_client.count(index=index_name)
        total_docs = index_stats['count']
        print(f"  ğŸ“Š ç´¢å¼•ä¸­çš„æ–‡æ¡£æ€»æ•°: {total_docs}")
        
        # å¦‚æœæ²¡æœ‰æ–‡æ¡£ï¼Œç›´æ¥è¿”å›
        if total_docs == 0:
            print(f"  âš  ç´¢å¼• {index_name} ä¸­æ²¡æœ‰æ–‡æ¡£ï¼")
            return []
        
        response = es_client.search(index=index_name, body=query, size=top_k)
        
        # è°ƒè¯•ï¼šè¾“å‡ºæœç´¢ç»“æœç»Ÿè®¡
        total_hits = response['hits']['total']
        if isinstance(total_hits, dict):
            total_hits = total_hits.get('value', 0)
        print(f"  ğŸ“Š åŒ¹é…åˆ°çš„æ–‡æ¡£æ•°: {total_hits}")
        
        results = []
        for hit in response['hits']['hits']:
            result = SearchResult(
                content=hit['_source'].get('content', ''),
                score=hit['_score'],
                metadata={
                    'domain': hit['_source'].get('domain', ''),
                    'doc_type': hit['_source'].get('doc_type', ''),
                    'source': hit['_source'].get('source', ''),
                    'region': hit['_source'].get('region', ''),
                    'publish_date': hit['_source'].get('publish_date', ''),
                    'doc_id': hit['_source'].get('doc_id', ''),
                },
                chunk_id=hit['_source'].get('chunk_id', hit['_id'])
            )
            results.append(result)
        
        # å¦‚æœæ²¡æœ‰ç»“æœï¼Œå°è¯•ä¸å¸¦è¿‡æ»¤æ¡ä»¶çš„æœç´¢ï¼ŒæŸ¥çœ‹ç´¢å¼•ä¸­æ˜¯å¦æœ‰æ•°æ®
        if len(results) == 0 and must_filters:
            print(f"  âš  å¸¦è¿‡æ»¤æ¡ä»¶æœªæ‰¾åˆ°ç»“æœï¼Œå°è¯•æŸ¥çœ‹ç´¢å¼•ä¸­çš„æ–‡æ¡£æ ·æœ¬...")
            sample_query = {
                "size": 3,
                "query": {"match_all": {}}
            }
            try:
                sample_response = es_client.search(index=index_name, body=sample_query)
                if sample_response['hits']['hits']:
                    print(f"  ğŸ“ ç´¢å¼•ä¸­çš„æ–‡æ¡£æ ·æœ¬ï¼ˆå‰3æ¡ï¼‰:")
                    for i, hit in enumerate(sample_response['hits']['hits'], 1):
                        source = hit['_source']
                        print(f"    æ–‡æ¡£{i}:")
                        print(f"      domain: {source.get('domain', 'N/A')}")
                        print(f"      role: {source.get('role', 'N/A')}")
                        print(f"      status: {source.get('status', 'N/A')}")
                        print(f"      content: {source.get('content', '')[:100]}...")
            except Exception as e:
                print(f"  âš  è·å–æ–‡æ¡£æ ·æœ¬å¤±è´¥: {e}")
        
        return results
        
    except Exception as e:
        raise Exception(f"Elasticsearchæœç´¢å¤±è´¥: {e}")

# ============================================================================
# é‡æ’åº
# ============================================================================

def rerank_results(
    query: str,
    results: List[SearchResult],
    top_k: int = None,
    method: str = None
) -> List[SearchResult]:
    """
    å¯¹æœç´¢ç»“æœè¿›è¡Œé‡æ’åº
    
    å‚æ•°:
        query: åŸå§‹æŸ¥è¯¢
        results: æœç´¢ç»“æœåˆ—è¡¨
        top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
        method: é‡æ’åºæ–¹æ³•ï¼ˆ'similarity' æˆ– 'bm25'ï¼‰
    
    è¿”å›:
        List[SearchResult]: é‡æ’åºåçš„ç»“æœ
    """
    config = get_rag_config()
    
    if top_k is None:
        top_k = config['rerank_top_k']
    
    if method is None:
        method = config['rerank_method']
    
    if not results:
        return []
    
    # æ–¹æ³•1ï¼šåŸºäºç›¸ä¼¼åº¦åˆ†æ•°æ’åºï¼ˆç®€å•ä½†æœ‰æ•ˆï¼‰
    if method == 'similarity':
        # æŒ‰åˆ†æ•°é™åºæ’åº
        sorted_results = sorted(results, key=lambda x: x.score, reverse=True)
        return sorted_results[:top_k]
    
    # æ–¹æ³•2ï¼šBM25åˆ†æ•°ï¼ˆéœ€è¦å®ç°BM25è®¡ç®—ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
    elif method == 'bm25':
        # ç®€åŒ–ç‰ˆï¼šç»“åˆç›¸ä¼¼åº¦åˆ†æ•°å’Œæ–‡æœ¬åŒ¹é…åº¦
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„BM25ç®—æ³•
        for result in results:
            # ç®€å•çš„æ–‡æœ¬åŒ¹é…åº¦è®¡ç®—
            query_words = set(query.lower().split())
            content_words = set(result.content.lower().split())
            match_ratio = len(query_words & content_words) / max(len(query_words), 1)
            # ç»¼åˆåˆ†æ•° = å‘é‡ç›¸ä¼¼åº¦ * 0.7 + æ–‡æœ¬åŒ¹é…åº¦ * 0.3
            result.score = result.score * 0.7 + match_ratio * 0.3
        
        sorted_results = sorted(results, key=lambda x: x.score, reverse=True)
        return sorted_results[:top_k]
    
    else:
        # é»˜è®¤ï¼šæŒ‰ç›¸ä¼¼åº¦æ’åº
        sorted_results = sorted(results, key=lambda x: x.score, reverse=True)
        return sorted_results[:top_k]

# ============================================================================
# å¤§æ¨¡å‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
# ============================================================================

def generate_answer(
    query: str,
    search_results: List[SearchResult],
    domain: str = 'general',
    stream: bool = False
) -> str:
    """
    ä½¿ç”¨å¤§æ¨¡å‹åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    
    å‚æ•°:
        query: ç”¨æˆ·æŸ¥è¯¢
        search_results: æœç´¢ç»“æœåˆ—è¡¨
        domain: åŸŸç±»å‹ï¼ˆ'policy'/'system'/'general'ï¼‰
        stream: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆé»˜è®¤Falseï¼‰
    
    è¿”å›:
        str: ç”Ÿæˆçš„ç­”æ¡ˆï¼ˆå¦‚æœstream=Trueï¼Œè¿”å›å®Œæ•´æ–‡æœ¬ï¼‰
    """
    if not search_results:
        return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•ä½¿ç”¨å…¶ä»–å…³é”®è¯æŸ¥è¯¢ã€‚"
    
    # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆåˆå¹¶æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼‰
    context_parts = []
    for i, result in enumerate(search_results, 1):
        # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
        metadata_info = []
        if result.metadata.get('source'):
            metadata_info.append(f"æ¥æºï¼š{result.metadata['source']}")
        if result.metadata.get('publish_date'):
            metadata_info.append(f"å‘å¸ƒæ—¶é—´ï¼š{result.metadata['publish_date']}")
        if result.metadata.get('region'):
            metadata_info.append(f"åœ°åŒºï¼š{result.metadata['region']}")
        
        metadata_str = " | ".join(metadata_info) if metadata_info else ""
        
        context_part = f"[æ–‡æ¡£{i}]"
        if metadata_str:
            context_part += f" ({metadata_str})"
        context_part += f"\n{result.content}\n"
        context_parts.append(context_part)
    
    context = "\n".join(context_parts)
    
    # è·å–æç¤ºè¯æ¨¡æ¿
    prompt_template = get_rag_query_prompt(domain)
    
    # å¡«å……æç¤ºè¯ï¼ˆåŒ…å«æ—¥æœŸä¿¡æ¯ï¼‰
    prompt = prompt_template.format(context=context, question=query, today=TODAY)
    
    # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
    try:
        answer = call_llm(prompt, module="rag_answer", stream=stream)
        return answer.strip()
    except Exception as e:
        return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}"


def generate_answer_stream(
    query: str,
    search_results: List[SearchResult],
    domain: str = 'general'
) -> Generator[str, None, None]:
    """
    ä½¿ç”¨å¤§æ¨¡å‹åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼ˆæµå¼è¾“å‡ºï¼‰
    
    å‚æ•°:
        query: ç”¨æˆ·æŸ¥è¯¢
        search_results: æœç´¢ç»“æœåˆ—è¡¨
        domain: åŸŸç±»å‹ï¼ˆ'policy'/'system'/'general'ï¼‰
    
    è¿”å›:
        Generator[str, None, None]: ç”Ÿæˆå™¨ï¼Œé€æ­¥è¿”å›ç­”æ¡ˆæ–‡æœ¬ç‰‡æ®µ
    """
    if not search_results:
        yield "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•ä½¿ç”¨å…¶ä»–å…³é”®è¯æŸ¥è¯¢ã€‚"
        return
    
    # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆåˆå¹¶æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼‰
    context_parts = []
    for i, result in enumerate(search_results, 1):
        # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
        metadata_info = []
        if result.metadata.get('source'):
            metadata_info.append(f"æ¥æºï¼š{result.metadata['source']}")
        if result.metadata.get('publish_date'):
            metadata_info.append(f"å‘å¸ƒæ—¶é—´ï¼š{result.metadata['publish_date']}")
        if result.metadata.get('region'):
            metadata_info.append(f"åœ°åŒºï¼š{result.metadata['region']}")
        
        metadata_str = " | ".join(metadata_info) if metadata_info else ""
        
        context_part = f"[æ–‡æ¡£{i}]"
        if metadata_str:
            context_part += f" ({metadata_str})"
        context_part += f"\n{result.content}\n"
        context_parts.append(context_part)
    
    context = "\n".join(context_parts)
    
    # è·å–æç¤ºè¯æ¨¡æ¿
    prompt_template = get_rag_query_prompt(domain)
    
    # å¡«å……æç¤ºè¯ï¼ˆåŒ…å«æ—¥æœŸä¿¡æ¯ï¼‰
    prompt = prompt_template.format(context=context, question=query, today=TODAY)
    
    # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆï¼ˆæµå¼è¾“å‡ºï¼‰
    try:
        print(f"[generate_answer_stream] å¼€å§‹è°ƒç”¨æµå¼LLMï¼Œprompté•¿åº¦: {len(prompt)}")
        chunk_count = 0
        for chunk in call_llm_stream(prompt, module="rag_answer"):
            chunk_count += 1
            if chunk_count <= 3:  # åªæ‰“å°å‰3ä¸ªchunkçš„æ—¥å¿—
                print(f"[generate_answer_stream] æ”¶åˆ°chunk #{chunk_count}: {chunk[:50]}...")
            yield chunk
        print(f"[generate_answer_stream] æµå¼ç”Ÿæˆå®Œæˆï¼Œæ€»å…±æ”¶åˆ° {chunk_count} ä¸ªchunk")
        if chunk_count == 0:
            print(f"[generate_answer_stream] âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æ”¶åˆ°ä»»ä½•chunk")
    except Exception as e:
        error_msg = f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}"
        print(f"[generate_answer_stream] å¼‚å¸¸: {error_msg}")
        yield error_msg

# ============================================================================
# RAGæŸ¥è¯¢ä¸»å‡½æ•°
# ============================================================================

def rag_query(
    query: str,
    domain: str = 'policy',
    role: str = 'å®¢æˆ·ç»ç†',
    enable_rewrite: bool = True,
    enable_rerank: bool = None,
    filters: Dict = None
) -> Dict:
    """
    RAGæŸ¥è¯¢ä¸»å‡½æ•°
    
    å‚æ•°:
        query: ç”¨æˆ·æŸ¥è¯¢
        domain: åŸŸç±»å‹ï¼ˆ'policy'/'system'ï¼‰
        role: ç”¨æˆ·è§’è‰²ï¼ˆç”¨äºæƒé™è¿‡æ»¤ï¼‰
        enable_rewrite: æ˜¯å¦å¯ç”¨queryæ”¹å†™
        enable_rerank: æ˜¯å¦å¯ç”¨é‡æ’åºï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼ï¼‰
        filters: é¢å¤–çš„è¿‡æ»¤æ¡ä»¶
    
    è¿”å›:
        Dict: åŒ…å«ç­”æ¡ˆå’Œæ£€ç´¢ç»“æœçš„å­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"RAGæŸ¥è¯¢")
    print(f"{'='*60}")
    print(f"åŸå§‹æŸ¥è¯¢: {query}")
    print(f"åŸŸç±»å‹: {domain}")
    print(f"ç”¨æˆ·è§’è‰²: {role}")
    
    # æ­¥éª¤1ï¼šQueryæ”¹å†™ï¼ˆå¦‚æœå¯ç”¨ä¸”æœªåœ¨å¤–éƒ¨æ”¹å†™ï¼‰
    if enable_rewrite:
        print(f"\n[æ­¥éª¤1] Queryæ”¹å†™...")
        # å¯¼å…¥æç¤ºè¯
        from config.prompts import QUERY_REWRITE_PROMPT, TODAY
        # æ„å»ºæç¤ºè¯å¹¶è°ƒç”¨LLMï¼ˆåŒ…å«æ—¥æœŸä¿¡æ¯ï¼‰
        prompt = QUERY_REWRITE_PROMPT.format(original_query=query, today=TODAY)
        rewritten_query = call_llm(prompt, module="rag_query_rewrite")
        rewritten_query = rewritten_query.strip().strip('"').strip("'")
        if not rewritten_query or len(rewritten_query) < 3:
            rewritten_query = query
        print(f"  æ”¹å†™å: {rewritten_query}")
        search_query = rewritten_query
    else:
        # å¦‚æœå·²åœ¨å¤–éƒ¨æ”¹å†™ï¼ˆå¦‚app.pyï¼‰ï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„query
        search_query = query
        rewritten_query = None
    
    # æ­¥éª¤2ï¼šå‘é‡åŒ–
    print(f"\n[æ­¥éª¤2] å‘é‡åŒ–æŸ¥è¯¢...")
    try:
        query_vector = generate_query_embedding(search_query)
        print(f"  âœ“ å‘é‡ç»´åº¦: {query_vector.shape[0]}")
    except Exception as e:
        raise Exception(f"å‘é‡åŒ–å¤±è´¥: {e}")
    
    # æ­¥éª¤3ï¼šElasticsearchå‘é‡æœç´¢
    print(f"\n[æ­¥éª¤3] Elasticsearchå‘é‡æœç´¢...")
    try:
        index_config = INDEX_CONFIG.get(domain)
        if not index_config:
            raise Exception(f"æœªæ‰¾åˆ°åŸŸ {domain} çš„ç´¢å¼•é…ç½®")
        
        index_name = index_config['index_name']
        config = get_rag_config()
        search_results = search_vectors(
            query_vector=query_vector,
            index_name=index_name,
            domain=domain,
            role=role,
            top_k=config['top_k'],
            filters=filters
        )
        print(f"  âœ“ æ£€ç´¢åˆ° {len(search_results)} æ¡ç»“æœ")
    except Exception as e:
        raise Exception(f"å‘é‡æœç´¢å¤±è´¥: {e}")
    
    # æ­¥éª¤4ï¼šé‡æ’åºï¼ˆå¯é€‰ï¼‰
    config = get_rag_config()
    
    if enable_rerank is None:
        enable_rerank = config['enable_rerank']
    
    if enable_rerank and search_results:
        print(f"\n[æ­¥éª¤4] é‡æ’åº...")
        search_results = rerank_results(
            query=search_query,
            results=search_results,
            top_k=config['rerank_top_k'],
            method=config['rerank_method']
        )
        print(f"  âœ“ é‡æ’åºåä¿ç•™ {len(search_results)} æ¡ç»“æœ")
    
    # è¿‡æ»¤ä½åˆ†ç»“æœ
    config = get_rag_config()
    filtered_results = [
        r for r in search_results 
        if r.score >= config['min_score']
    ]
    
    if not filtered_results:
        return {
            'answer': "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•ä½¿ç”¨å…¶ä»–å…³é”®è¯æŸ¥è¯¢ã€‚",
            'query': query,
            'rewritten_query': rewritten_query if enable_rewrite else None,
            'results': [],
            'domain': domain
        }
    
    # æ­¥éª¤5ï¼šå¤§æ¨¡å‹ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    print(f"\n[æ­¥éª¤5] å¤§æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ...")
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æµå¼è¾“å‡ºï¼ˆä»…å¯¹æœ€åä¸€æ¬¡LLMæ€»ç»“ç»“æœè°ƒç”¨ä½¿ç”¨æµå¼ï¼‰
    enable_streaming = RAG_CONFIG.get('enable_streaming', True)
    answer = generate_answer(
        query=query,  # ä½¿ç”¨åŸå§‹æŸ¥è¯¢ï¼Œä¸æ˜¯æ”¹å†™åçš„
        search_results=filtered_results,
        domain=domain,
        stream=enable_streaming
    )
    print(f"  âœ“ ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
    
    return {
        'answer': answer,
        'query': query,
        'rewritten_query': rewritten_query if enable_rewrite else None,
        'results': [
            {
                'content': r.content,
                'score': r.score,
                'metadata': r.metadata,
                'chunk_id': r.chunk_id
            }
            for r in filtered_results
        ],
        'domain': domain
    }


# ============================================================================
# ä¸»å‡½æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
# ============================================================================

def main():
    """æµ‹è¯•å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='RAGæŸ¥è¯¢æµ‹è¯•')
    parser.add_argument('query', type=str, help='æŸ¥è¯¢é—®é¢˜')
    parser.add_argument('--domain', type=str, default='policy', choices=['policy', 'system'],
                       help='åŸŸç±»å‹ï¼ˆpolicy/systemï¼‰')
    parser.add_argument('--role', type=str, default='å®¢æˆ·ç»ç†',
                       help='ç”¨æˆ·è§’è‰²ï¼ˆå®¢æˆ·ç»ç†/å›¢é˜Ÿè´Ÿè´£äºº/è¡Œé•¿ï¼‰')
    parser.add_argument('--no-rewrite', action='store_true',
                       help='ç¦ç”¨queryæ”¹å†™')
    parser.add_argument('--no-rerank', action='store_true',
                       help='ç¦ç”¨é‡æ’åº')
    
    args = parser.parse_args()
    
    try:
        result = rag_query(
            query=args.query,
            domain=args.domain,
            role=args.role,
            enable_rewrite=not args.no_rewrite,
            enable_rerank=not args.no_rerank
        )
        
        print(f"\n{'='*60}")
        print(f"æŸ¥è¯¢ç»“æœ")
        print(f"{'='*60}")
        print(f"\nç­”æ¡ˆï¼š\n{result['answer']}")
        
        if result['rewritten_query']:
            print(f"\næ”¹å†™åçš„æŸ¥è¯¢ï¼š{result['rewritten_query']}")
        
        print(f"\næ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡ï¼š{len(result['results'])}")
        for i, r in enumerate(result['results'][:3], 1):  # åªæ˜¾ç¤ºå‰3æ¡
            print(f"\n[æ–‡æ¡£{i}] ç›¸ä¼¼åº¦: {r['score']:.4f}")
            print(f"  å†…å®¹: {r['content'][:100]}...")
            print(f"  æ¥æº: {r['metadata'].get('source', 'æœªçŸ¥')}")
        
    except Exception as e:
        print(f"\nâŒ æŸ¥è¯¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

