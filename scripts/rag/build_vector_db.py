"""
RAGå‘é‡åº“ç”Ÿæˆè„šæœ¬
åŠŸèƒ½ï¼š
1. è¯»å–QAå¯¹JSONæ–‡ä»¶ï¼ˆdata/qa_pairs/ï¼‰
2. å°†QAå¯¹è½¬æ¢ä¸ºæ–‡æ¡£å—æ ¼å¼
3. ç”ŸæˆEmbeddingå‘é‡
4. ä¿å­˜åˆ°Elasticsearchï¼ˆå•ä¸ªæ··åˆç´¢å¼•ï¼ŒåŒæ—¶åŒ…å«å‘é‡å’Œæ–‡æœ¬å­—æ®µï¼Œæ”¯æŒESåŸç”ŸHybrid Searchï¼‰

Metadataè®¾è®¡ï¼ˆç¬¦åˆéœ€æ±‚æ–‡æ¡£ï¼‰ï¼š
- domain: åŸŸç±»å‹ï¼ˆæ”¿ç­– | ç³»ç»Ÿï¼‰
- doc_type: æ–‡æ¡£ç±»å‹ï¼ˆç›‘ç®¡æ”¿ç­– | å†…éƒ¨åˆ¶åº¦ | ç³»ç»Ÿè¯´æ˜ | æ“ä½œæ‰‹å†Œï¼‰
- source: æ¥æºæœºæ„ï¼ˆå›½å®¶é‡‘èç›‘ç£ç®¡ç†æ€»å±€ç­‰ï¼‰
- region: åœ°åŒºèŒƒå›´ï¼ˆå…¨å›½ | æ–°ç–† ç­‰ï¼‰
- publish_date: å‘å¸ƒæ—¶é—´ï¼ˆYYYY-MM-DDæ ¼å¼ï¼‰
- status: çŠ¶æ€ï¼ˆç”Ÿæ•ˆ | å·²å¤±æ•ˆï¼‰
- doc_id: æ–‡æ¡£ç¼–å·
- role: æƒé™è§’è‰²ï¼ˆå®¢æˆ·ç»ç† | å›¢é˜Ÿè´Ÿè´£äºº | è¡Œé•¿ï¼Œç”¨äºæŸ¥è¯¢æƒé™æ§åˆ¶ï¼‰
"""

import os
import json
import sys
import time
from pathlib import Path
from typing import List, Dict
from datetime import datetime
import hashlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥Elasticsearché…ç½®
from config.elasticsearch import ES_CONFIG, INDEX_CONFIG, BATCH_SIZE

# ============================================================================
# é…ç½®åŒºåŸŸ
# ============================================================================

# Embeddingæ¨¡å‹é…ç½®
# ä½¿ç”¨ModelScopeçš„Qwen3-Embeddingæ¨¡å‹
# å¯é€‰æ¨¡å‹ï¼š
# - Qwen/Qwen3-Embedding-0.6B (0.6Bå‚æ•°ï¼Œé€Ÿåº¦å¿«ï¼Œé€‚åˆå¼€å‘æµ‹è¯•)
# - Qwen/Qwen3-Embedding-8B (8Bå‚æ•°ï¼Œæ•ˆæœå¥½ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ)
EMBEDDING_MODEL_NAME = os.getenv('EMBEDDING_MODEL', 'Qwen/Qwen3-Embedding-0.6B')
# ModelScopeæ¨¡å‹è·¯å¾„æ ¼å¼ï¼šQwen/Qwen3-Embedding-0.6B æˆ– Qwen/Qwen3-Embedding-8B

# Embeddingæ‰¹å¤„ç†é…ç½®ï¼ˆé’ˆå¯¹ä½é…ç½®æœºå™¨ä¼˜åŒ–ï¼‰
# å¦‚æœæœºå™¨é…ç½®è¾ƒä½ï¼Œå¯ä»¥å‡å°è¿™ä¸ªå€¼ï¼ˆå»ºè®®4-8ï¼‰
EMBEDDING_BATCH_SIZE = int(os.getenv('EMBEDDING_BATCH_SIZE', '4'))  # é»˜è®¤4ï¼Œä½é…ç½®æœºå™¨æ›´å‹å¥½

# QAå¯¹æ–‡ä»¶è·¯å¾„
QA_PAIRS_DIR = project_root / "data" / "qa_pairs"

# ============================================================================
# ä¾èµ–æ£€æŸ¥
# ============================================================================

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    print("âš  è­¦å‘Š: numpyæœªå®‰è£…ï¼Œå°†æ— æ³•ç”Ÿæˆå‘é‡åº“")
    print("   è¯·å®‰è£…: pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple")

try:
    from modelscope import snapshot_download
    from transformers import AutoTokenizer, AutoModel
    import torch
    MODELSCOPE_AVAILABLE = True
except ImportError:
    MODELSCOPE_AVAILABLE = False
    print("âš  è­¦å‘Š: modelscopeæˆ–transformersæœªå®‰è£…ï¼Œå°†æ— æ³•ç”Ÿæˆembedding")
    print("   è¯·å®‰è£…: pip install modelscope transformers torch -i https://pypi.tuna.tsinghua.edu.cn/simple")

try:
    from elasticsearch import Elasticsearch
    from elasticsearch.helpers import bulk
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False
    print("âš  è­¦å‘Š: elasticsearchæœªå®‰è£…ï¼Œå°†æ— æ³•ä¿å­˜åˆ°Elasticsearch")
    print("   è¯·å®‰è£…: pip install elasticsearch -i https://pypi.tuna.tsinghua.edu.cn/simple")


# ============================================================================
# Embeddingç”Ÿæˆå‡½æ•°
# ============================================================================

def load_embedding_model(model_path: str):
    """
    åŠ è½½ModelScopeçš„Qwen3-Embeddingæ¨¡å‹
    
    å‚æ•°:
        model_path: æ¨¡å‹è·¯å¾„ï¼ˆModelScopeæ ¼å¼ï¼‰
    
    è¿”å›:
        tuple: (model, tokenizer)
    """
    print(f"  æ­£åœ¨ä»ModelScopeåŠ è½½æ¨¡å‹: {model_path}")
    
    # ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœæœªä¸‹è½½ï¼‰
    try:
        model_dir = snapshot_download(model_path, cache_dir='./models/embedding')
        print(f"  âœ“ æ¨¡å‹è·¯å¾„: {model_dir}")
    except Exception as e:
        print(f"  âš  æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        raise
    
    # åŠ è½½tokenizerå’Œmodel
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰GPUå¯ç”¨ï¼Œå¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œä½¿ç”¨CPU
        use_cpu = os.getenv('FORCE_CPU', 'false').lower() == 'true'
        if use_cpu:
            print(f"  â„¹ å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼ï¼ˆç¯å¢ƒå˜é‡FORCE_CPU=trueï¼‰")
            device_map = 'cpu'
        elif torch.cuda.is_available():
            # æ£€æŸ¥GPUå†…å­˜
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            print(f"  â„¹ æ£€æµ‹åˆ°GPUï¼Œæ˜¾å­˜: {gpu_memory:.1f}GB")
            if gpu_memory < 4:  # å¦‚æœæ˜¾å­˜å°äº4GBï¼Œå»ºè®®ä½¿ç”¨CPU
                print(f"  âš  æ˜¾å­˜è¾ƒå°ï¼Œå»ºè®®è®¾ç½®ç¯å¢ƒå˜é‡ FORCE_CPU=true ä½¿ç”¨CPUæ¨¡å¼")
            device_map = 'auto'
        else:
            print(f"  â„¹ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")
            device_map = 'cpu'
        
        model = AutoModel.from_pretrained(model_dir, trust_remote_code=True, device_map=device_map)
        model.eval()
        
        # æ˜¾ç¤ºæ¨¡å‹å®é™…ä½¿ç”¨çš„è®¾å¤‡
        actual_device = next(model.parameters()).device
        print(f"  âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {actual_device}")
        return model, tokenizer
    except Exception as e:
        print(f"  âš  æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"    æç¤º: å¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œå¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡ FORCE_CPU=true å¼ºåˆ¶ä½¿ç”¨CPU")
        raise


def generate_embeddings(texts: List[str], model, tokenizer, batch_size: int = 4) -> np.ndarray:
    """
    ä½¿ç”¨Qwen3-Embeddingç”Ÿæˆæ–‡æœ¬çš„embeddingå‘é‡
    
    å‚æ•°:
        texts: æ–‡æœ¬åˆ—è¡¨
        model: Qwen3-Embeddingæ¨¡å‹
        tokenizer: å¯¹åº”çš„tokenizer
        batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤4ï¼Œé€‚åˆä½é…ç½®æœºå™¨ï¼‰
    
    è¿”å›:
        np.ndarray: embeddingå‘é‡çŸ©é˜µ
    """
    all_embeddings = []
    
    total_batches = (len(texts) + batch_size - 1) // batch_size  # è®¡ç®—æ€»æ‰¹æ¬¡æ•°
    print(f"  æ­£åœ¨ç”Ÿæˆembeddingå‘é‡ï¼ˆå…± {len(texts)} æ¡æ–‡æœ¬ï¼Œåˆ† {total_batches} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} æ¡ï¼‰...")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # åˆ†æ‰¹å¤„ç†
    for batch_idx, i in enumerate(range(0, len(texts), batch_size), 1):
        batch_texts = texts[i:i + batch_size]
        batch_start_time = time.time()
        
        try:
            # Tokenizeï¼ˆæ–‡æœ¬ç¼–ç ï¼‰
            inputs = tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            )
            
            # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
            device = next(model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ç”Ÿæˆembeddingï¼ˆæ¨¡å‹æ¨ç†ï¼‰
            with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜
                outputs = model(**inputs)
                # Qwen3-Embeddingå¯èƒ½è¿”å›ä¸åŒçš„æ ¼å¼
                # ä¼˜å…ˆä½¿ç”¨pooler_outputï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨mean pooling
                if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
                    embeddings = outputs.pooler_output.cpu().numpy()
                elif hasattr(outputs, 'last_hidden_state'):
                    # ä½¿ç”¨mean poolingè·å–å¥å­çº§åˆ«çš„embedding
                    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
                else:
                    # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨outputs
                    embeddings = outputs.cpu().numpy() if isinstance(outputs, torch.Tensor) else outputs
                    if isinstance(embeddings, torch.Tensor):
                        embeddings = embeddings.numpy()
            
            all_embeddings.append(embeddings)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            batch_time = time.time() - batch_start_time
            elapsed_time = time.time() - start_time
            
            # æ¯æ‰¹éƒ½æ˜¾ç¤ºè¿›åº¦ï¼ˆè®©ç”¨æˆ·çŸ¥é“ç¨‹åºåœ¨è¿è¡Œï¼‰
            processed_count = min(i + batch_size, len(texts))
            progress = (processed_count / len(texts)) * 100
            avg_time_per_batch = elapsed_time / batch_idx
            estimated_remaining = avg_time_per_batch * (total_batches - batch_idx)
            
            print(f"    [{batch_idx}/{total_batches}] å·²å¤„ç† {processed_count}/{len(texts)} æ¡ "
                  f"({progress:.1f}%) | æœ¬æ‰¹è€—æ—¶: {batch_time:.1f}ç§’ | "
                  f"é¢„è®¡å‰©ä½™: {estimated_remaining:.0f}ç§’")
            
            # æ¸…ç†GPUç¼“å­˜ï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰
            if device.type == 'cuda':
                torch.cuda.empty_cache()
                
        except Exception as e:
            print(f"    âš  å¤„ç†ç¬¬ {batch_idx} æ‰¹æ—¶å‡ºé”™: {e}")
            print(f"    æç¤º: å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥å‡å° EMBEDDING_BATCH_SIZE ç¯å¢ƒå˜é‡ï¼ˆå½“å‰: {batch_size}ï¼‰")
            raise
    
    # åˆå¹¶æ‰€æœ‰embeddings
    final_embeddings = np.vstack(all_embeddings)
    total_time = time.time() - start_time
    avg_time_per_text = total_time / len(texts)
    
    print(f"  âœ“ ç”Ÿæˆäº† {len(final_embeddings)} ä¸ªembeddingå‘é‡ (ç»´åº¦: {final_embeddings.shape[1]})")
    print(f"  âœ“ æ€»è€—æ—¶: {total_time:.1f}ç§’ï¼Œå¹³å‡æ¯æ¡: {avg_time_per_text:.2f}ç§’")
    
    return final_embeddings


# ============================================================================
# Elasticsearchå‘é‡åº“ä¿å­˜å‡½æ•°
# ============================================================================

def test_elasticsearch_connection() -> bool:
    """
    æµ‹è¯•Elasticsearchè¿æ¥æ˜¯å¦æ­£å¸¸
    
    è¿”å›:
        bool: è¿æ¥æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    if not ELASTICSEARCH_AVAILABLE:
        print("âš  é”™è¯¯: elasticsearchåº“æœªå®‰è£…")
        print("   è¯·å®‰è£…: pip install elasticsearch -i https://pypi.tuna.tsinghua.edu.cn/simple")
        return False
    
    print("\næ­£åœ¨æµ‹è¯•Elasticsearchè¿æ¥...")
    print(f"  è¿æ¥åœ°å€: {ES_CONFIG.get('hosts', ['æœªçŸ¥'])}")
    print(f"  SSLéªŒè¯: {ES_CONFIG.get('verify_certs', True)}")
    # æ˜¾ç¤ºè®¤è¯ä¿¡æ¯ï¼ˆä¸æ˜¾ç¤ºå¯†ç ï¼‰
    if 'basic_auth' in ES_CONFIG:
        username = ES_CONFIG['basic_auth'][0] if isinstance(ES_CONFIG['basic_auth'], tuple) else 'å·²é…ç½®'
        print(f"  è®¤è¯ç”¨æˆ·: {username}")
    else:
        print(f"  è®¤è¯æ–¹å¼: æ— è®¤è¯")
    
    try:
        # åˆ›å»ºElasticsearchå®¢æˆ·ç«¯
        es_client = Elasticsearch(**ES_CONFIG)
        
        # å°è¯•è·å–é›†ç¾¤ä¿¡æ¯ï¼ˆæ¯”pingæ›´å¯é ï¼‰
        try:
            info = es_client.info()
            print(f"  âœ“ Elasticsearchè¿æ¥æˆåŠŸï¼")
            print(f"    é›†ç¾¤åç§°: {info.get('cluster_name', 'æœªçŸ¥')}")
            print(f"    ç‰ˆæœ¬: {info.get('version', {}).get('number', 'æœªçŸ¥')}")
            print(f"    èŠ‚ç‚¹åç§°: {info.get('name', 'æœªçŸ¥')}")
            return True
        except Exception as info_error:
            # å¦‚æœinfo()å¤±è´¥ï¼Œå°è¯•ping()
            print(f"  âš  è·å–é›†ç¾¤ä¿¡æ¯å¤±è´¥: {info_error}")
            try:
                if es_client.ping():
                    print(f"  âœ“ pingæˆåŠŸï¼Œä½†æ— æ³•è·å–è¯¦ç»†ä¿¡æ¯")
                    return True
                else:
                    print(f"  âš  pingä¹Ÿå¤±è´¥")
            except Exception as ping_error:
                print(f"  âš  pingå¤±è´¥: {ping_error}")
            
            return False
        
    except Exception as e:
        error_msg = str(e)
        print(f"  âš  Elasticsearchè¿æ¥å¤±è´¥: {error_msg}")
        
        # æ ¹æ®é”™è¯¯ç±»å‹ç»™å‡ºå…·ä½“å»ºè®®
        if 'SSL' in error_msg or 'certificate' in error_msg.lower():
            print(f"\n  SSLè¯ä¹¦é”™è¯¯ï¼Œå°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆ:")
            print(f"    1. ç¡®è®¤ESä½¿ç”¨HTTPSï¼ˆå½“å‰é…ç½®: {ES_CONFIG.get('hosts')}ï¼‰")
            print(f"    2. å¦‚æœESä½¿ç”¨HTTPï¼Œè¯·ä¿®æ”¹ config/elasticsearch.py:")
            print(f"       'hosts': ['http://localhost:9200']")
            print(f"    3. å¦‚æœæ˜¯è‡ªç­¾åè¯ä¹¦ï¼Œç¡®ä¿ verify_certs=False")
        elif 'Connection' in error_msg or 'refused' in error_msg.lower():
            print(f"\n  è¿æ¥è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥:")
            print(f"    1. ElasticsearchæœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ")
            print(f"    2. ç«¯å£9200æ˜¯å¦è¢«å ç”¨")
            print(f"    3. é˜²ç«å¢™æ˜¯å¦é˜»æ­¢äº†è¿æ¥")
        else:
            print(f"\n  è¯·æ£€æŸ¥:")
            print(f"    1. ElasticsearchæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
            print(f"    2. é…ç½®ä¸­çš„åœ°å€å’Œç«¯å£æ˜¯å¦æ­£ç¡®ï¼ˆå½“å‰: {ES_CONFIG.get('hosts', ['æœªçŸ¥'])}ï¼‰")
            print(f"    3. æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„åè®®ï¼ˆhttp:// æˆ– https://ï¼‰")
            print(f"    4. å¦‚æœæ˜¯HTTPSï¼Œè¯ä¹¦æ˜¯å¦æ­£ç¡®ï¼ˆå½“å‰verify_certs={ES_CONFIG.get('verify_certs', True)}ï¼‰")
        
        return False


def create_vector_index(es_client: Elasticsearch, index_name: str, vector_dimension: int):
    """
    åˆ›å»ºæ”¯æŒHybrid Searchçš„æ··åˆç´¢å¼•ï¼ˆåŒæ—¶æ”¯æŒå‘é‡æœç´¢å’Œæ–‡æœ¬æœç´¢ï¼‰
    
    å‚æ•°:
        es_client: Elasticsearchå®¢æˆ·ç«¯
        index_name: ç´¢å¼•åç§°
        vector_dimension: å‘é‡ç»´åº¦
    
    åŠŸèƒ½:
        - å®šä¹‰ç´¢å¼•æ˜ å°„ï¼ŒåŒ…å«dense_vectorå­—æ®µç”¨äºå‘é‡æœç´¢ï¼ˆKNNï¼‰
        - åŒæ—¶åŒ…å«æ–‡æœ¬å­—æ®µç”¨äºæ–‡æœ¬æœç´¢ï¼ˆBM25ï¼‰
        - æ”¯æŒESåŸç”ŸHybrid Searchï¼ˆBM25 + å‘é‡æœç´¢ï¼‰
        - åˆ›å»ºç´¢å¼•ï¼ˆå¦‚æœå·²å­˜åœ¨åˆ™åˆ é™¤é‡å»ºï¼‰
    """
    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åˆ é™¤
    if es_client.indices.exists(index=index_name):
        print(f"  ç´¢å¼• {index_name} å·²å­˜åœ¨ï¼Œåˆ é™¤æ—§ç´¢å¼•...")
        es_client.indices.delete(index=index_name)
    
    # é€‰æ‹©åˆ†è¯å™¨ï¼ˆä¼˜å…ˆä½¿ç”¨IKåˆ†è¯å™¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨ï¼‰
    analyzer_name = "ik_max_word"
    search_analyzer_name = "ik_max_word"
    
    # å®šä¹‰ç´¢å¼•æ˜ å°„ï¼ˆå‘é‡æ¨¡å¼ï¼ŒåŒ…å«å‘é‡å­—æ®µå’Œæ–‡æœ¬å­—æ®µï¼‰
    index_mapping = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
        },
        "mappings": {
            "properties": {
                # æ–‡æ¡£æ ‡é¢˜å­—æ®µï¼ˆæ–‡æœ¬æœç´¢ï¼‰
                "title": {
                    "type": "text",
                    "analyzer": analyzer_name,
                    "search_analyzer": search_analyzer_name,
                    "fields": {
                        "keyword": {
                            "type": "keyword"
                        }
                    }
                },
                # æ–‡æ¡£æºæ–‡ä»¶è·¯å¾„
                "source": {
                    "type": "keyword"
                },
                # æ–‡æ¡£å—å†…å®¹ï¼ˆæ–‡æœ¬æœç´¢ï¼‰
                "content": {
                    "type": "text",
                    "analyzer": analyzer_name,
                    "search_analyzer": search_analyzer_name
                },
                # å‘é‡å­—æ®µï¼ˆç”¨äºå‘é‡æœç´¢ï¼‰
                "embedding": {
                    "type": "dense_vector",      # å¯†é›†å‘é‡ç±»å‹
                    "dims": vector_dimension,    # å‘é‡ç»´åº¦
                    "index": True,               # å¯ç”¨å‘é‡ç´¢å¼•
                    "similarity": "cosine"       # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
                },
                # å—ID
                "chunk_id": {
                    "type": "keyword"
                },
                # é¡µç 
                "page_num": {
                    "type": "integer"
                },
                # Tokenæ•°é‡
                "token_count": {
                    "type": "integer"
                },
                # æ–‡æ¡£ç±»å‹
                "file_type": {
                    "type": "keyword"
                },
                # Metadataå­—æ®µï¼ˆç”¨äºè¿‡æ»¤ï¼‰
                "domain": {
                    "type": "keyword"
                },
                "doc_type": {
                    "type": "keyword"
                },
                "region": {
                    "type": "keyword"
                },
                "publish_date": {
                    "type": "date",
                    "format": "yyyy-MM-dd"
                },
                "status": {
                    "type": "keyword"
                },
                "role": {
                    "type": "keyword"
                },
                "doc_id": {
                    "type": "keyword"
                },
                # QAå¯¹ç›¸å…³å­—æ®µ
                "qa_id": {
                    "type": "keyword"
                },
                "question": {
                    "type": "text",
                    "analyzer": analyzer_name,
                    "search_analyzer": search_analyzer_name
                },
                "answer": {
                    "type": "text",
                    "analyzer": analyzer_name,
                    "search_analyzer": search_analyzer_name
                }
            }
        }
    }
    
    # åˆ›å»ºç´¢å¼•
    try:
        es_client.indices.create(
            index=index_name,
            settings=index_mapping["settings"],
            mappings=index_mapping["mappings"]
        )
        print(f"  âœ“ æˆåŠŸåˆ›å»ºæ··åˆç´¢å¼•: {index_name}")
        print(f"  å‘é‡ç»´åº¦: {vector_dimension}")
        print(f"  ç›¸ä¼¼åº¦ç®—æ³•: cosineï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰")
        print(f"  åˆ†è¯å™¨: {analyzer_name}")
        print(f"  æ”¯æŒåŠŸèƒ½: å‘é‡æœç´¢ï¼ˆKNNï¼‰+ æ–‡æœ¬æœç´¢ï¼ˆBM25ï¼‰+ æ··åˆæœç´¢ï¼ˆHybrid Searchï¼‰")
    except Exception as e:
        # å¦‚æœIKåˆ†è¯å™¨ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨
        error_msg = str(e)
        if "ik" in error_msg.lower() or "analyzer" in error_msg.lower():
            print(f"  âš  IKåˆ†è¯å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨")
            analyzer_name = "standard"
            search_analyzer_name = "standard"
            index_mapping["mappings"]["properties"]["title"]["analyzer"] = analyzer_name
            index_mapping["mappings"]["properties"]["title"]["search_analyzer"] = search_analyzer_name
            index_mapping["mappings"]["properties"]["content"]["analyzer"] = analyzer_name
            index_mapping["mappings"]["properties"]["content"]["search_analyzer"] = search_analyzer_name
            index_mapping["mappings"]["properties"]["question"]["analyzer"] = analyzer_name
            index_mapping["mappings"]["properties"]["question"]["search_analyzer"] = search_analyzer_name
            index_mapping["mappings"]["properties"]["answer"]["analyzer"] = analyzer_name
            index_mapping["mappings"]["properties"]["answer"]["search_analyzer"] = search_analyzer_name
            
            es_client.indices.create(
                index=index_name,
                settings=index_mapping["settings"],
                mappings=index_mapping["mappings"]
            )
            print(f"  âœ“ æˆåŠŸåˆ›å»ºæ··åˆç´¢å¼•: {index_name}ï¼ˆä½¿ç”¨æ ‡å‡†åˆ†è¯å™¨ï¼‰")
            print(f"  æ”¯æŒåŠŸèƒ½: å‘é‡æœç´¢ï¼ˆKNNï¼‰+ æ–‡æœ¬æœç´¢ï¼ˆBM25ï¼‰+ æ··åˆæœç´¢ï¼ˆHybrid Searchï¼‰")
        else:
            print(f"  âœ— åˆ›å»ºç´¢å¼•å¤±è´¥: {error_msg}")
            raise


def index_documents_text(es_client: Elasticsearch, index_name: str, chunks: List[Dict]):
    """
    å°†æ–‡æ¡£å—ç´¢å¼•åˆ°Elasticsearchï¼ˆéå‘é‡æ¨¡å¼ï¼Œåªä¿å­˜æ–‡æœ¬æ•°æ®ï¼‰
    
    å‚æ•°:
        es_client: Elasticsearchå®¢æˆ·ç«¯å¯¹è±¡
        index_name: ç´¢å¼•åç§°
        chunks: æ–‡æ¡£å—åˆ—è¡¨
    
    åŠŸèƒ½:
        - ä½¿ç”¨bulk APIæ‰¹é‡ç´¢å¼•æ–‡æ¡£
        - ä¸ºæ¯ä¸ªæ–‡æ¡£å—ç”Ÿæˆå”¯ä¸€ID
        - æ˜¾ç¤ºç´¢å¼•è¿›åº¦
    """
    if not chunks:
        print("âš  æ²¡æœ‰æ–‡æ¡£å—éœ€è¦ç´¢å¼•")
        return
    
    # å‡†å¤‡æ‰¹é‡ç´¢å¼•çš„æ•°æ®
    actions = []
    for i, chunk in enumerate(chunks):
        # ç”Ÿæˆæ–‡æ¡£ID
        doc_id = f"{hashlib.sha256(chunk['source'].encode()).hexdigest()[:16]}_{chunk['chunk_id']}"
        
        # æ„å»ºè¦ç´¢å¼•çš„æ–‡æ¡£ï¼ˆä¸åŒ…å«å‘é‡å­—æ®µï¼‰
        action = {
            "_index": index_name,
            "_id": doc_id,
            "_source": chunk
        }
        actions.append(action)
    
    print(f"ğŸ“¤ å‡†å¤‡ç´¢å¼• {len(actions)} ä¸ªæ–‡æ¡£å—ï¼ˆéå‘é‡æ¨¡å¼ï¼‰...")
    
    try:
        # ä½¿ç”¨bulk APIæ‰¹é‡ç´¢å¼•
        success_count, failed_items = bulk(es_client, actions, chunk_size=BATCH_SIZE, request_timeout=60)
        
        print(f"âœ“ ç´¢å¼•å®Œæˆï¼")
        print(f"  æˆåŠŸç´¢å¼•: {success_count} ä¸ªæ–‡æ¡£")
        if failed_items:
            print(f"  å¤±è´¥: {len(failed_items)} ä¸ªæ–‡æ¡£")
            for item in failed_items[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå¤±è´¥é¡¹
                print(f"    - {item}")
        
        # åˆ·æ–°ç´¢å¼•ï¼Œä½¿æ–°ç´¢å¼•çš„æ–‡æ¡£ç«‹å³å¯æœç´¢
        es_client.indices.refresh(index=index_name)
        print(f"  ç´¢å¼•å·²åˆ·æ–°ï¼Œæ–‡æ¡£å¯ç«‹å³æœç´¢")
        
    except Exception as e:
        print(f"âœ— ç´¢å¼•å¤±è´¥: {str(e)}")
        raise


def index_documents_with_vectors(es_client: Elasticsearch, index_name: str, chunks: List[Dict], 
                                  embeddings: np.ndarray):
    """
    å°†æ–‡æ¡£å—å’Œå‘é‡ç´¢å¼•åˆ°Elasticsearchï¼ˆæ··åˆç´¢å¼•ï¼ŒåŒæ—¶åŒ…å«å‘é‡å’Œæ–‡æœ¬å­—æ®µï¼‰
    
    å‚æ•°:
        es_client: Elasticsearchå®¢æˆ·ç«¯å¯¹è±¡
        index_name: ç´¢å¼•åç§°
        chunks: æ–‡æ¡£å—åˆ—è¡¨
        embeddings: å‘é‡çŸ©é˜µï¼ˆnumpyæ•°ç»„ï¼‰
    
    åŠŸèƒ½:
        - ä¸ºæ¯ä¸ªæ–‡æ¡£å—ç”Ÿæˆå‘é‡åµŒå…¥
        - ä½¿ç”¨bulk APIæ‰¹é‡ç´¢å¼•æ–‡æ¡£å’Œå‘é‡
        - å•ä¸ªç´¢å¼•åŒæ—¶åŒ…å«å‘é‡å­—æ®µå’Œæ–‡æœ¬å­—æ®µï¼Œæ”¯æŒHybrid Search
        - æ˜¾ç¤ºç´¢å¼•è¿›åº¦
    """
    if not chunks:
        print("âš  æ²¡æœ‰æ–‡æ¡£å—éœ€è¦ç´¢å¼•")
        return
    
    if embeddings is None or len(embeddings) == 0:
        print("âš  æ²¡æœ‰å‘é‡æ•°æ®éœ€è¦ç´¢å¼•")
        return
    
    if len(chunks) != len(embeddings):
        raise Exception(f"æ–‡æ¡£å—æ•°é‡({len(chunks)})ä¸å‘é‡æ•°é‡({len(embeddings)})ä¸åŒ¹é…")
    
    # å‡†å¤‡æ‰¹é‡ç´¢å¼•çš„æ•°æ®
    actions = []
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        # ç”Ÿæˆæ–‡æ¡£ID
        doc_id = f"{hashlib.sha256(chunk['source'].encode()).hexdigest()[:16]}_{chunk['chunk_id']}"
        
        # æ„å»ºè¦ç´¢å¼•çš„æ–‡æ¡£ï¼ˆåŒ…å«å‘é‡å­—æ®µï¼‰
        doc_data = chunk.copy()
        doc_data['embedding'] = embedding.tolist()  # numpyæ•°ç»„è½¬åˆ—è¡¨
        
        action = {
            "_index": index_name,
            "_id": doc_id,
            "_source": doc_data
        }
        actions.append(action)
    
    print(f"ğŸ“¤ å‡†å¤‡ç´¢å¼• {len(actions)} ä¸ªæ–‡æ¡£å—ï¼ˆåŒ…å«å‘é‡ï¼‰...")
    
    try:
        # æ‰¹é‡ç´¢å¼•ï¼ˆå‘é‡æ•°æ®è¾ƒå¤§ï¼Œä½¿ç”¨è¾ƒå°çš„chunk_sizeï¼‰
        success_count, failed_items = bulk(es_client, actions, chunk_size=50, request_timeout=120)
        
        print(f"âœ“ ç´¢å¼•å®Œæˆï¼")
        print(f"  æˆåŠŸç´¢å¼•: {success_count} ä¸ªæ–‡æ¡£")
        if failed_items:
            print(f"  å¤±è´¥: {len(failed_items)} ä¸ªæ–‡æ¡£")
            for item in failed_items[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå¤±è´¥é¡¹
                print(f"    - {item}")
        
        # åˆ·æ–°ç´¢å¼•
        es_client.indices.refresh(index=index_name)
        print(f"  ç´¢å¼•å·²åˆ·æ–°ï¼Œæ–‡æ¡£å¯ç«‹å³æœç´¢")
        
    except Exception as e:
        print(f"âœ— ç´¢å¼•å¤±è´¥: {str(e)}")
        raise


# ============================================================================
# QAå¯¹æ•°æ®è¯»å–å’Œè½¬æ¢
# ============================================================================

def prepare_chunks_from_qa_pairs(domain: str = 'system') -> List[Dict]:
    """
    ä»QAå¯¹JSONæ–‡ä»¶ä¸­è¯»å–æ•°æ®å¹¶è½¬æ¢ä¸ºæ–‡æ¡£å—æ ¼å¼
    æ”¯æŒè¯»å–å¤šä¸ªQAå¯¹æ–‡ä»¶ï¼ˆæ¯ä¸ªæ–‡æ¡£ä¸€ä¸ªæ–‡ä»¶ï¼‰
    
    å‚æ•°:
        domain: åŸŸç±»å‹ï¼ˆpolicy/systemï¼‰
    
    è¿”å›:
        æ–‡æ¡£å—åˆ—è¡¨ï¼Œæ¯ä¸ªå—åŒ…å«title, source, contentç­‰ä¿¡æ¯
    
    åŠŸèƒ½:
        - è¯»å– data/qa_pairs/{domain}/*.json æ‰€æœ‰æ–‡ä»¶
        - å°†æ¯ä¸ªQAå¯¹è½¬æ¢ä¸ºæ–‡æ¡£å—æ ¼å¼
        - ä½¿ç”¨questionå’Œanswerç»„åˆä½œä¸ºcontent
        - ä¿ç•™æ¯ä¸ªæ–‡æ¡£çš„metadataä¿¡æ¯
    """
    # æ„å»ºQAå¯¹ç›®å½•è·¯å¾„
    qa_dir = QA_PAIRS_DIR / domain
    
    if not qa_dir.exists():
        raise FileNotFoundError(f"QAå¯¹ç›®å½•ä¸å­˜åœ¨: {qa_dir}")
    
    # æŸ¥æ‰¾æ‰€æœ‰ *.json æ–‡ä»¶
    qa_files = list(qa_dir.glob('*.json'))
    
    if not qa_files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°JSONæ–‡ä»¶: {qa_dir}/*.json")
    
    print(f"ğŸ“ æ‰¾åˆ° {len(qa_files)} ä¸ªJSONæ–‡ä»¶")
    
    all_chunks = []
    total_qa_pairs = 0
    
    # éå†æ‰€æœ‰QAå¯¹æ–‡ä»¶
    for qa_file in qa_files:
        print(f"  æ­£åœ¨è¯»å–: {qa_file.name}")
        
        try:
            # è¯»å–JSONæ–‡ä»¶
            with open(qa_file, 'r', encoding='utf-8') as f:
                qa_pairs = json.load(f)
            
            if not isinstance(qa_pairs, list):
                print(f"    âš  è·³è¿‡: æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼ˆä¸æ˜¯æ•°ç»„ï¼‰")
                continue
            
            print(f"    âœ“ è¯»å– {len(qa_pairs)} ä¸ªQAå¯¹")
            total_qa_pairs += len(qa_pairs)
            
            # å°†QAå¯¹è½¬æ¢ä¸ºæ–‡æ¡£å—æ ¼å¼
            for idx, qa in enumerate(qa_pairs):
                # æå–QAå¯¹ä¿¡æ¯
                qa_id = qa.get('id', f'{domain}_{idx:04d}')
                question = qa.get('question', '')
                answer = qa.get('answer', '')
                created_at = qa.get('created_at', '')
                
                # ä»metadataä¸­æå–ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                doc_id = qa.get('doc_id', f"{domain}_{qa_id}")
                doc_type = qa.get('doc_type', 'æ“ä½œæ‰‹å†Œ' if domain == 'system' else 'ç›‘ç®¡æ”¿ç­–')
                region = qa.get('region', 'å…¨å›½')
                status = qa.get('status', 'ç”Ÿæ•ˆ')
                role = qa.get('role', 'å®¢æˆ·ç»ç†')
                domain_value = qa.get('domain', 'ç³»ç»Ÿ' if domain == 'system' else 'æ”¿ç­–')
                
                # ç»„åˆquestionå’Œanswerä½œä¸ºcontentï¼ˆä¾¿äºæœç´¢ï¼‰
                # æ ¼å¼ï¼šé—®é¢˜ï¼šxxx\nç­”æ¡ˆï¼šxxx
                content = f"é—®é¢˜ï¼š{question}\nç­”æ¡ˆï¼š{answer}"
                
                # è®¡ç®—tokenæ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼š1 token â‰ˆ 4å­—ç¬¦ï¼‰
                token_count = len(content) // 4
                
                # æ„å»ºæ–‡æ¡£å—æ•°æ®
                chunk = {
                    'title': question[:100] if len(question) > 100 else question,  # ä½¿ç”¨é—®é¢˜ä½œä¸ºæ ‡é¢˜
                    'source': str(qa_file),  # æºæ–‡ä»¶è·¯å¾„
                    'content': content,      # é—®é¢˜å’Œç­”æ¡ˆçš„ç»„åˆå†…å®¹
                    'chunk_id': len(all_chunks),  # å—IDï¼ˆå…¨å±€ç´¢å¼•ï¼‰
                    'page_num': 1,           # é¡µç ï¼ˆQAå¯¹é€šå¸¸ä¸åˆ†é¡µï¼‰
                    'token_count': token_count,  # Tokenæ•°é‡
                    'file_type': 'json',     # æ–‡ä»¶ç±»å‹
                    'domain': domain_value,  # åŸŸç±»å‹ï¼ˆä»metadataè·å–ï¼‰
                    'doc_type': doc_type,     # æ–‡æ¡£ç±»å‹ï¼ˆä»metadataè·å–ï¼‰
                    'region': region,        # åœ°åŒºèŒƒå›´ï¼ˆä»metadataè·å–ï¼‰
                    'publish_date': created_at[:10] if created_at else '2024-01-01',  # å‘å¸ƒæ—¶é—´ï¼ˆæå–æ—¥æœŸéƒ¨åˆ†ï¼‰
                    'status': status,        # çŠ¶æ€ï¼ˆä»metadataè·å–ï¼‰
                    'role': role,            # è§’è‰²ï¼ˆä»metadataè·å–ï¼‰
                    'doc_id': doc_id,        # æ–‡æ¡£IDï¼ˆä»metadataè·å–ï¼‰
                    # é¢å¤–å­—æ®µï¼šä¿å­˜åŸå§‹QAå¯¹ä¿¡æ¯
                    'qa_id': qa_id,         # QAå¯¹ID
                    'question': question,    # åŸå§‹é—®é¢˜
                    'answer': answer        # åŸå§‹ç­”æ¡ˆ
                }
                all_chunks.append(chunk)
        
        except json.JSONDecodeError as e:
            print(f"    âš  JSONè§£æå¤±è´¥: {e}")
            continue
        except Exception as e:
            print(f"    âš  è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            continue
    
    print(f"\nâœ“ æˆåŠŸè¯»å– {len(qa_files)} ä¸ªæ–‡ä»¶ï¼Œå…± {total_qa_pairs} ä¸ªQAå¯¹")
    print(f"âœ“ æˆåŠŸè½¬æ¢ {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
    
    if all_chunks:
        print(f"  ç¤ºä¾‹ï¼š")
        print(f"    - æ ‡é¢˜: {all_chunks[0]['title']}")
        print(f"    - å†…å®¹é•¿åº¦: {len(all_chunks[0]['content'])} å­—ç¬¦")
        print(f"    - Tokenæ•°: {all_chunks[0]['token_count']}")
        print(f"    - æ–‡æ¡£ID: {all_chunks[0]['doc_id']}")
    
    return all_chunks


# ============================================================================
# ä¸»å¤„ç†å‡½æ•°
# ============================================================================

def process_domain(domain: str, model, tokenizer):
    """
    å¤„ç†æŸä¸ªåŸŸçš„QAå¯¹æ•°æ®ï¼Œç”Ÿæˆå‘é‡åº“
    
    å‚æ•°:
        domain: åŸŸç±»å‹ï¼ˆpolicy/systemï¼‰
        model: Embeddingæ¨¡å‹
        tokenizer: Tokenizer
    """
    print("=" * 60)
    print(f"å¤„ç† {domain.upper()} åŸŸQAå¯¹æ•°æ®")
    print("=" * 60)
    
    # æ£€æŸ¥QAå¯¹ç›®å½•æ˜¯å¦å­˜åœ¨
    qa_dir = QA_PAIRS_DIR / domain
    if not qa_dir.exists():
        print(f"âš  è·³è¿‡: QAå¯¹ç›®å½•ä¸å­˜åœ¨: {qa_dir}")
        return
    
    # æ£€æŸ¥æ˜¯å¦æœ‰JSONæ–‡ä»¶
    qa_files = list(qa_dir.glob('*.json'))
    if not qa_files:
        print(f"âš  è·³è¿‡: æœªæ‰¾åˆ°JSONæ–‡ä»¶: {qa_dir}/*.json")
        return
    
    # ä»QAå¯¹æ–‡ä»¶è¯»å–æ•°æ®
    chunks = prepare_chunks_from_qa_pairs(domain)
    
    if not chunks:
        print("âš  æ²¡æœ‰å¯ç”¨çš„æ–‡æ¡£å—")
        return
    
    print(f"\næ€»å…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æ¡£å—")
    
    # ç”Ÿæˆembeddingsï¼ˆä½¿ç”¨é…ç½®çš„batch_sizeï¼‰
    texts = [chunk['content'] for chunk in chunks]
    embeddings = generate_embeddings(texts, model, tokenizer, batch_size=EMBEDDING_BATCH_SIZE)
    
    # è¿æ¥Elasticsearch
    try:
        es_client = Elasticsearch(**ES_CONFIG)
        # å†æ¬¡æµ‹è¯•è¿æ¥ï¼ˆç¡®ä¿è¿æ¥æ­£å¸¸ï¼‰
        if not es_client.ping():
            raise Exception("æ— æ³•è¿æ¥åˆ°ElasticsearchæœåŠ¡å™¨")
    except Exception as e:
        print(f"  âš  Elasticsearchè¿æ¥å¤±è´¥: {e}")
        raise
    
    # è·å–ç´¢å¼•é…ç½®
    index_config = INDEX_CONFIG.get(domain)
    if not index_config:
        print(f"  âš  æœªæ‰¾åˆ°åŸŸ {domain} çš„ç´¢å¼•é…ç½®")
        return
    
    index_name = index_config['index_name']
    vector_dimension = embeddings.shape[1]
    
    # æ›´æ–°å‘é‡ç»´åº¦ï¼ˆå¦‚æœä¸é…ç½®ä¸ä¸€è‡´ï¼‰
    if vector_dimension != index_config['vector_dimension']:
        print(f"  âš  å‘é‡ç»´åº¦ {vector_dimension} ä¸é…ç½®ä¸ä¸€è‡´ï¼Œæ›´æ–°é…ç½®")
        index_config['vector_dimension'] = vector_dimension
    
    # åˆ›å»ºæ··åˆç´¢å¼•ï¼ˆåŒæ—¶æ”¯æŒå‘é‡æœç´¢å’Œæ–‡æœ¬æœç´¢ï¼Œç”¨äºESåŸç”ŸHybrid Searchï¼‰
    print(f"\næ­£åœ¨åˆ›å»ºæ··åˆç´¢å¼•ï¼ˆæ”¯æŒHybrid Searchï¼‰: {index_name}")
    create_vector_index(es_client, index_name, vector_dimension)
    
    # ç´¢å¼•æ–‡æ¡£å’Œå‘é‡ï¼ˆå•ä¸ªç´¢å¼•åŒ…å«å‘é‡å’Œæ–‡æœ¬å­—æ®µï¼Œæ”¯æŒæ··åˆæœç´¢ï¼‰
    print(f"\næ­£åœ¨ç´¢å¼•æ•°æ®ï¼ˆåŒ…å«å‘é‡å’Œæ–‡æœ¬å­—æ®µï¼‰...")
    index_documents_with_vectors(es_client, index_name, chunks, embeddings)
    
    # æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
    stats = es_client.count(index=index_name)
    print(f"âœ“ æ··åˆç´¢å¼• {index_name} ä¸­å…±æœ‰ {stats['count']} æ¡æ–‡æ¡£")
    print(f"  - æ”¯æŒå‘é‡æœç´¢ï¼ˆKNNï¼‰")
    print(f"  - æ”¯æŒæ–‡æœ¬æœç´¢ï¼ˆBM25ï¼‰")
    print(f"  - æ”¯æŒæ··åˆæœç´¢ï¼ˆHybrid Searchï¼šBM25 + å‘é‡ï¼‰")
    
    print(f"\nâœ“ {domain} åŸŸå‘é‡åº“ç”Ÿæˆå®Œæˆï¼")


def main():
    """
    ä¸»å‡½æ•°
    """
    print("=" * 60)
    print("RAGå‘é‡åº“ç”Ÿæˆè„šæœ¬ï¼ˆåŸºäºQAå¯¹æ•°æ®ï¼‰")
    print("=" * 60)
    
    # æ£€æŸ¥ä¾èµ–
    if not NUMPY_AVAILABLE:
        print("âš  é”™è¯¯: numpyæœªå®‰è£…")
        return
    
    if not MODELSCOPE_AVAILABLE:
        print("âš  é”™è¯¯: modelscopeæˆ–transformersæœªå®‰è£…")
        return
    
    if not ELASTICSEARCH_AVAILABLE:
        print("âš  é”™è¯¯: elasticsearchæœªå®‰è£…")
        return
    
    # å…ˆæµ‹è¯•Elasticsearchè¿æ¥ï¼ˆåœ¨åŠ è½½æ¨¡å‹ä¹‹å‰ï¼Œé¿å…æµªè´¹èµ„æºï¼‰
    if not test_elasticsearch_connection():
        print("\nâš  é”™è¯¯: Elasticsearchè¿æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        print("   è¯·å…ˆè§£å†³Elasticsearchè¿æ¥é—®é¢˜ï¼Œç„¶åå†è¿è¡Œè„šæœ¬")
        return
    
    # åŠ è½½embeddingæ¨¡å‹ï¼ˆè¿æ¥æˆåŠŸåå†åŠ è½½ï¼Œé¿å…æµªè´¹èµ„æºï¼‰
    print(f"\næ­£åœ¨åŠ è½½embeddingæ¨¡å‹: {EMBEDDING_MODEL_NAME}...")
    try:
        model, tokenizer = load_embedding_model(EMBEDDING_MODEL_NAME)
    except Exception as e:
        print(f"âš  æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    print()
    
    # å¤„ç†policyå’Œsystemä¸¤ä¸ªåŸŸ
    domains = ['policy', 'system']
    
    for domain in domains:
        # å¤„ç†è¯¥åŸŸçš„QAå¯¹æ•°æ®
        try:
            process_domain(domain, model, tokenizer)
        except Exception as e:
            print(f"âš  å¤„ç† {domain} åŸŸå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        if domain != domains[-1]:
            print("\n" + "="*60 + "\n")
    
    print("\n" + "=" * 60)
    print("å‘é‡åº“ç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)
    print("\nç´¢å¼•è¯´æ˜ï¼š")
    print(f"  æ”¿ç­–ç±»ï¼š")
    print(f"    - {INDEX_CONFIG['policy']['index_name']}: æ··åˆç´¢å¼•ï¼ˆæ”¯æŒHybrid Searchï¼‰")
    print(f"  ç³»ç»ŸåŠŸèƒ½ç±»ï¼š")
    print(f"    - {INDEX_CONFIG['system']['index_name']}: æ··åˆç´¢å¼•ï¼ˆæ”¯æŒHybrid Searchï¼‰")
    print("\næŸ¥è¯¢ç¤ºä¾‹ï¼š")
    print("  # æ··åˆæœç´¢ï¼ˆHybrid Searchï¼šBM25 + å‘é‡ï¼‰")
    print("  POST /bank_credit_policy/_search")
    print("  # å‘é‡æœç´¢ï¼ˆKNNï¼‰")
    print("  POST /bank_credit_policy/_search")
    print("  # æ–‡æœ¬æœç´¢ï¼ˆBM25ï¼‰")
    print("  POST /bank_credit_policy/_search")


if __name__ == "__main__":
    main()
