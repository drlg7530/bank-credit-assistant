"""
Elasticsearchæ•°æ®ä¿å­˜å’ŒæŸ¥è¯¢æµ‹è¯•è„šæœ¬

åŠŸèƒ½ï¼š
1. ä¿å­˜æ··åˆæ•°æ®åˆ°Elasticsearchï¼ˆåŒæ—¶åŒ…å«å‘é‡å’Œæ–‡æœ¬å­—æ®µï¼‰
2. æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•ï¼ˆæ”¯æŒæ–‡æœ¬æœç´¢ã€å‘é‡æœç´¢å’Œæ··åˆæœç´¢ï¼‰

ä½¿ç”¨è¯´æ˜ï¼š
1. æ··åˆç´¢å¼•ï¼šå•ä¸ªç´¢å¼•åŒæ—¶åŒ…å«å‘é‡å­—æ®µå’Œæ–‡æœ¬å­—æ®µ
2. æ–‡æœ¬æœç´¢ï¼šä½¿ç”¨BM25ç®—æ³•è¿›è¡Œå…¨æ–‡æœç´¢
3. å‘é‡æœç´¢ï¼šä½¿ç”¨KNNç®—æ³•è¿›è¡Œç›¸ä¼¼åº¦æœç´¢
4. æ··åˆæœç´¢ï¼šESåŸç”ŸHybrid Searchï¼ˆBM25 + å‘é‡æœç´¢ï¼‰

å‚è€ƒä»£ç ï¼š
- cankao/elasticsearch_index_search.py çš„ index_documents() å‡½æ•°
- cankao/es_doc_search_embedding.py çš„ index_documents_with_vectors() å‡½æ•°
"""

import sys
import os
import argparse
import hashlib
from pathlib import Path
from typing import List, Dict, Optional
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é…ç½®
from config.elasticsearch import ES_CONFIG, INDEX_CONFIG, BATCH_SIZE
from config.rag_config import get_rag_config

# å¯¼å…¥å¿…è¦çš„å‡½æ•°
from scripts.rag.build_vector_db import (
    test_elasticsearch_connection,
    load_embedding_model,
    generate_embeddings,
    create_vector_index,
    index_documents_with_vectors
)

# Embeddingæ¨¡å‹é…ç½®ï¼ˆä¸build_vector_db.pyä¿æŒä¸€è‡´ï¼‰
EMBEDDING_MODEL_NAME = os.getenv('EMBEDDING_MODEL', 'Qwen/Qwen3-Embedding-0.6B')

from src.rag.query import (
    search_vectors,
    generate_query_embedding,
    SearchResult
)

# ============================================================================
# ä¾èµ–æ£€æŸ¥
# ============================================================================

try:
    from elasticsearch import Elasticsearch
    from elasticsearch.helpers import bulk
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False
    print("âš  è­¦å‘Š: elasticsearchæœªå®‰è£…")
    print("   è¯·å®‰è£…: pip install elasticsearch -i https://pypi.tuna.tsinghua.edu.cn/simple")

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    print("âš  è­¦å‘Š: numpyæœªå®‰è£…")

# ============================================================================
# æ­¥éª¤1ï¼šè¿æ¥Elasticsearch
# ============================================================================

def connect_elasticsearch() -> Elasticsearch:
    """
    è¿æ¥åˆ°ElasticsearchæœåŠ¡å™¨
    
    åŠŸèƒ½ï¼š
    - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è¿æ¥ä¿¡æ¯
    - æµ‹è¯•è¿æ¥æ˜¯å¦æˆåŠŸ
    
    è¿”å›ï¼š
    - Elasticsearchå®¢æˆ·ç«¯å¯¹è±¡
    """
    print("\n" + "="*60)
    print("æ­¥éª¤1ï¼šè¿æ¥Elasticsearch")
    print("="*60)
    
    if not ELASTICSEARCH_AVAILABLE:
        raise Exception("elasticsearchåº“æœªå®‰è£…")
    
    try:
        # åˆ›å»ºElasticsearchå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é…ç½®ï¼‰
        es_client = Elasticsearch(**ES_CONFIG)
        
        # æµ‹è¯•è¿æ¥
        cluster_info = es_client.info()
        print(f"âœ“ æˆåŠŸè¿æ¥åˆ°Elasticsearch")
        print(f"  é›†ç¾¤åç§°: {cluster_info.get('cluster_name', 'æœªçŸ¥')}")
        print(f"  ç‰ˆæœ¬: {cluster_info.get('version', {}).get('number', 'æœªçŸ¥')}")
        return es_client
    except Exception as e:
        print(f"âœ— è¿æ¥å¤±è´¥: {e}")
        raise


# æ³¨æ„ï¼šcreate_vector_index å’Œ index_documents_with_vectors å‡½æ•°å·²ä» build_vector_db.py å¯¼å…¥


# ============================================================================
# æ­¥éª¤4ï¼šå‡†å¤‡æ–‡æ¡£å—æ•°æ®ï¼ˆä»QAå¯¹JSONæ–‡ä»¶è¯»å–ï¼‰
# ============================================================================

def prepare_chunks_from_qa_pairs(domain: str = 'system') -> List[Dict]:
    """
    ä»QAå¯¹JSONæ–‡ä»¶ä¸­è¯»å–æ•°æ®å¹¶è½¬æ¢ä¸ºæ–‡æ¡£å—æ ¼å¼
    
    å‚æ•°ï¼š
    - domain: åŸŸç±»å‹ï¼ˆpolicy/systemï¼‰
    
    è¿”å›ï¼š
    - æ–‡æ¡£å—åˆ—è¡¨ï¼Œæ¯ä¸ªå—åŒ…å«title, source, contentç­‰ä¿¡æ¯
    
    åŠŸèƒ½ï¼š
    - è¯»å– data/qa_pairs/{domain}/{domain}_qa_pairs.json æ–‡ä»¶
    - å°†æ¯ä¸ªQAå¯¹è½¬æ¢ä¸ºæ–‡æ¡£å—æ ¼å¼
    - ä½¿ç”¨questionå’Œanswerç»„åˆä½œä¸ºcontent
    """
    print("\n" + "="*60)
    print("æ­¥éª¤4ï¼šä»QAå¯¹JSONæ–‡ä»¶è¯»å–æ–‡æ¡£å—æ•°æ®")
    print("="*60)
    
    # æ„å»ºQAå¯¹JSONæ–‡ä»¶è·¯å¾„
    qa_file = project_root / "data" / "qa_pairs" / domain / f"{domain}_qa_pairs.json"
    
    if not qa_file.exists():
        raise FileNotFoundError(f"QAå¯¹æ–‡ä»¶ä¸å­˜åœ¨: {qa_file}")
    
    print(f"ğŸ“ æ­£åœ¨è¯»å–QAå¯¹æ–‡ä»¶: {qa_file}")
    
    try:
        import json
        # è¯»å–JSONæ–‡ä»¶
        with open(qa_file, 'r', encoding='utf-8') as f:
            qa_pairs = json.load(f)
        
        print(f"âœ“ æˆåŠŸè¯»å– {len(qa_pairs)} ä¸ªQAå¯¹")
        
        # å°†QAå¯¹è½¬æ¢ä¸ºæ–‡æ¡£å—æ ¼å¼
        chunks = []
        for idx, qa in enumerate(qa_pairs):
            # æå–QAå¯¹ä¿¡æ¯
            qa_id = qa.get('id', f'{domain}_{idx:04d}')
            question = qa.get('question', '')
            answer = qa.get('answer', '')
            created_at = qa.get('created_at', '')
            
            # ç»„åˆquestionå’Œanswerä½œä¸ºcontentï¼ˆä¾¿äºæœç´¢ï¼‰
            # æ ¼å¼ï¼šé—®é¢˜ï¼šxxx\nç­”æ¡ˆï¼šxxx
            content = f"é—®é¢˜ï¼š{question}\nç­”æ¡ˆï¼š{answer}"
            
            # è®¡ç®—tokenæ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼š1 token â‰ˆ 4å­—ç¬¦ï¼‰
            token_count = len(content) // 4
            
            # æ„å»ºæ–‡æ¡£å—æ•°æ®
            chunk = {
                'title': question[:100] if len(question) > 100 else question,  # ä½¿ç”¨é—®é¢˜ä½œä¸ºæ ‡é¢˜
                'source': str(qa_file),  # æºæ–‡ä»¶è·¯å¾„
                'content': content,      # é—®é¢˜å’Œç­”æ¡ˆçš„ç»„åˆå†…å®¹
                'chunk_id': idx,         # å—IDï¼ˆä½¿ç”¨ç´¢å¼•ï¼‰
                'page_num': 1,           # é¡µç ï¼ˆQAå¯¹é€šå¸¸ä¸åˆ†é¡µï¼‰
                'token_count': token_count,  # Tokenæ•°é‡
                'file_type': 'json',     # æ–‡ä»¶ç±»å‹
                'domain': 'ç³»ç»Ÿ' if domain == 'system' else 'æ”¿ç­–',  # åŸŸç±»å‹ï¼ˆä¸­æ–‡ï¼‰
                'doc_type': 'æ“ä½œæ‰‹å†Œ' if domain == 'system' else 'ç›‘ç®¡æ”¿ç­–',  # æ–‡æ¡£ç±»å‹
                'region': 'å…¨å›½',        # åœ°åŒºèŒƒå›´
                'publish_date': created_at[:10] if created_at else '2024-01-01',  # å‘å¸ƒæ—¶é—´ï¼ˆæå–æ—¥æœŸéƒ¨åˆ†ï¼‰
                'status': 'ç”Ÿæ•ˆ',        # çŠ¶æ€
                'role': 'å®¢æˆ·ç»ç†',      # é»˜è®¤è§’è‰²
                # é¢å¤–å­—æ®µï¼šä¿å­˜åŸå§‹QAå¯¹ä¿¡æ¯
                'qa_id': qa_id,         # QAå¯¹ID
                'question': question,    # åŸå§‹é—®é¢˜
                'answer': answer         # åŸå§‹ç­”æ¡ˆ
            }
            chunks.append(chunk)
        
        print(f"âœ“ æˆåŠŸè½¬æ¢ {len(chunks)} ä¸ªæ–‡æ¡£å—")
        print(f"  ç¤ºä¾‹ï¼š")
        if chunks:
            print(f"    - æ ‡é¢˜: {chunks[0]['title']}")
            print(f"    - å†…å®¹é•¿åº¦: {len(chunks[0]['content'])} å­—ç¬¦")
            print(f"    - Tokenæ•°: {chunks[0]['token_count']}")
        
        return chunks
        
    except json.JSONDecodeError as e:
        raise Exception(f"JSONæ–‡ä»¶è§£æå¤±è´¥: {e}")
    except Exception as e:
        raise Exception(f"è¯»å–QAå¯¹æ–‡ä»¶å¤±è´¥: {e}")


# æ³¨æ„ï¼šindex_documents_with_vectors å‡½æ•°å·²ä» build_vector_db.py å¯¼å…¥


# ============================================================================
# æ­¥éª¤7ï¼šæ–‡æœ¬æœç´¢æµ‹è¯•
# ============================================================================

def test_text_search(es: Elasticsearch, index_name: str, query_text: str, top_k: int = 10):
    """
    æµ‹è¯•æ–‡æœ¬æœç´¢åŠŸèƒ½ï¼ˆéå‘é‡æ¨¡å¼ï¼‰
    
    å‚æ•°ï¼š
    - es: Elasticsearchå®¢æˆ·ç«¯å¯¹è±¡
    - index_name: ç´¢å¼•åç§°
    - query_text: æœç´¢æŸ¥è¯¢æ–‡æœ¬
    - top_k: è¿”å›å‰Kä¸ªç»“æœ
    
    åŠŸèƒ½ï¼š
    - ä½¿ç”¨multi_matchæŸ¥è¯¢åœ¨titleå’Œcontentå­—æ®µä¸­æœç´¢
    - è¿”å›ç›¸å…³æ€§è¯„åˆ†æœ€é«˜çš„æ–‡æ¡£
    """
    print("\n" + "="*60)
    print("æ­¥éª¤7ï¼šæ–‡æœ¬æœç´¢æµ‹è¯•")
    print("="*60)
    
    print(f"ğŸ” æœç´¢æŸ¥è¯¢: {query_text}")
    print(f"   è¿”å›å‰ {top_k} ä¸ªç»“æœ")
    
    # æ„å»ºæœç´¢æŸ¥è¯¢
    # ä½¿ç”¨multi_matchæŸ¥è¯¢ï¼Œåœ¨titleå’Œcontentå­—æ®µä¸­æœç´¢
    search_body = {
        "query": {
            "multi_match": {
                "query": query_text,              # æœç´¢æŸ¥è¯¢
                "fields": ["title^2", "content"], # æœç´¢å­—æ®µï¼Œtitleæƒé‡ä¸º2ï¼ˆæ›´é‡è¦ï¼‰
                "type": "best_fields"             # æœ€ä½³å­—æ®µåŒ¹é…
            }
        },
        "highlight": {  # é«˜äº®æ˜¾ç¤ºåŒ¹é…çš„æ–‡æœ¬
            "fields": {
                "title": {},
                "content": {
                    "fragment_size": 200,      # ç‰‡æ®µå¤§å°
                    "number_of_fragments": 3   # è¿”å›çš„ç‰‡æ®µæ•°
                }
            },
            "pre_tags": ["<mark>"],   # é«˜äº®å¼€å§‹æ ‡ç­¾
            "post_tags": ["</mark>"]  # é«˜äº®ç»“æŸæ ‡ç­¾
        },
        "size": top_k  # è¿”å›ç»“æœæ•°é‡
    }
    
    try:
        # æ‰§è¡Œæœç´¢
        response = es.search(
            index=index_name,
            query=search_body["query"],
            highlight=search_body["highlight"],
            size=search_body["size"]
        )
        
        # è§£ææœç´¢ç»“æœ
        hits = response['hits']['hits']
        total = response['hits']['total']['value']
        
        print(f"\nâœ“ æœç´¢å®Œæˆï¼")
        print(f"  æ‰¾åˆ° {total} ä¸ªç›¸å…³æ–‡æ¡£")
        print(f"  è¿”å›å‰ {len(hits)} ä¸ªç»“æœ\n")
        
        # æ˜¾ç¤ºç»“æœ
        if hits:
            for i, hit in enumerate(hits, 1):
                score = hit['_score']
                source = hit['_source']
                highlight = hit.get('highlight', {})
                
                print(f"[ç»“æœ {i}] ç›¸ä¼¼åº¦: {score:.4f}")
                print(f"  æ ‡é¢˜: {source.get('title', 'æ— æ ‡é¢˜')}")
                print(f"  æ¥æº: {source.get('source', 'æœªçŸ¥')}")
                print(f"  å†…å®¹: {source.get('content', '')[:200]}...")
                if highlight:
                    print(f"  é«˜äº®: {highlight}")
                print()
        else:
            print("  âš  æœªæ‰¾åˆ°åŒ¹é…çš„ç»“æœ")
        
        return hits
        
    except Exception as e:
        print(f"âœ— æœç´¢å¤±è´¥: {str(e)}")
        raise


# ============================================================================
# æ­¥éª¤8ï¼šå‘é‡æœç´¢æµ‹è¯•
# ============================================================================

def test_vector_search(query_text: str, domain: str, role: str = 'å®¢æˆ·ç»ç†', top_k: int = 10):
    """
    æµ‹è¯•å‘é‡æœç´¢åŠŸèƒ½ï¼ˆå‘é‡æ¨¡å¼ï¼‰
    
    å‚æ•°ï¼š
    - query_text: æŸ¥è¯¢æ–‡æœ¬
    - domain: åŸŸç±»å‹
    - role: ç”¨æˆ·è§’è‰²
    - top_k: è¿”å›å‰Kä¸ªç»“æœ
    
    åŠŸèƒ½ï¼š
    - å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
    - ä½¿ç”¨knnæŸ¥è¯¢è¿›è¡Œå‘é‡æœç´¢
    """
    print("\n" + "="*60)
    print("æ­¥éª¤8ï¼šå‘é‡æœç´¢æµ‹è¯•")
    print("="*60)
    
    print(f"æŸ¥è¯¢æ–‡æœ¬: {query_text}")
    print(f"åŸŸç±»å‹: {domain}")
    print(f"ç”¨æˆ·è§’è‰²: {role}")
    
    # è·å–ç´¢å¼•é…ç½®
    index_config = INDEX_CONFIG.get(domain)
    if not index_config:
        print(f"  âš  æœªæ‰¾åˆ°åŸŸ {domain} çš„ç´¢å¼•é…ç½®")
        return
    
    index_name = index_config['index_name']
    
    # å‘é‡åŒ–æŸ¥è¯¢
    print(f"\n[æŸ¥è¯¢æ­¥éª¤1] å‘é‡åŒ–æŸ¥è¯¢...")
    try:
        query_vector = generate_query_embedding(query_text)
        print(f"  âœ“ å‘é‡ç»´åº¦: {query_vector.shape[0]}")
    except Exception as e:
        print(f"  âš  å‘é‡åŒ–å¤±è´¥: {e}")
        return
    
    # æ‰§è¡Œæœç´¢
    print(f"\n[æŸ¥è¯¢æ­¥éª¤2] Elasticsearchå‘é‡æœç´¢...")
    try:
        config = get_rag_config()
        results = search_vectors(
            query_vector=query_vector,
            index_name=index_name,
            domain=domain,
            role=role,
            top_k=top_k or config['top_k']
        )
        
        print(f"\nâœ“ æ£€ç´¢åˆ° {len(results)} æ¡ç»“æœ\n")
        
        # æ˜¾ç¤ºç»“æœ
        if results:
            print(f"{'='*60}")
            print(f"æŸ¥è¯¢ç»“æœ")
            print(f"{'='*60}")
            for i, result in enumerate(results, 1):
                print(f"\n[ç»“æœ {i}] ç›¸ä¼¼åº¦: {result.score:.4f}")
                print(f"  å†…å®¹: {result.content[:200]}...")
                print(f"  å…ƒæ•°æ®: {result.metadata}")
        else:
            print("  âš  æœªæ‰¾åˆ°åŒ¹é…çš„ç»“æœ")
            print(f"\n  ğŸ’¡ æç¤ºï¼šç´¢å¼• {index_name} ä¸­æ²¡æœ‰æ–‡æ¡£ï¼Œéœ€è¦å…ˆæ„å»ºå‘é‡åº“")
            
    except Exception as e:
        print(f"  âš  æœç´¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def test_hybrid_search(es: Elasticsearch, index_name: str, query_text: str, query_vector: np.ndarray, top_k: int = 10):
    """
    æµ‹è¯•æ··åˆæœç´¢åŠŸèƒ½ï¼ˆHybrid Searchï¼šBM25 + å‘é‡æœç´¢ï¼‰
    
    å‚æ•°ï¼š
    - es: Elasticsearchå®¢æˆ·ç«¯å¯¹è±¡
    - index_name: ç´¢å¼•åç§°
    - query_text: æœç´¢æŸ¥è¯¢æ–‡æœ¬
    - query_vector: æŸ¥è¯¢å‘é‡
    - top_k: è¿”å›å‰Kä¸ªç»“æœ
    
    åŠŸèƒ½ï¼š
    - åŒæ—¶ä½¿ç”¨BM25æ–‡æœ¬æœç´¢å’ŒKNNå‘é‡æœç´¢
    - ESåŸç”ŸHybrid Searchä¼šè‡ªåŠ¨åˆå¹¶ä¸¤ç§æœç´¢ç»“æœ
    """
    print("\n" + "="*60)
    print("æ­¥éª¤9ï¼šæ··åˆæœç´¢æµ‹è¯•ï¼ˆHybrid Searchï¼šBM25 + å‘é‡ï¼‰")
    print("="*60)
    
    print(f"ğŸ” æœç´¢æŸ¥è¯¢: {query_text}")
    print(f"   è¿”å›å‰ {top_k} ä¸ªç»“æœ")
    print(f"   æœç´¢æ¨¡å¼: æ··åˆæœç´¢ï¼ˆBM25 + KNNå‘é‡æœç´¢ï¼‰")
    
    # æ„å»ºæ··åˆæœç´¢æŸ¥è¯¢
    # ESåŸç”ŸHybrid Searchï¼šåŒæ—¶åŒ…å«queryï¼ˆBM25ï¼‰å’Œknnï¼ˆå‘é‡æœç´¢ï¼‰
    search_body = {
        # å‘é‡æœç´¢ï¼ˆKNNï¼‰
        "knn": {
            "field": "embedding",
            "query_vector": query_vector.tolist(),
            "k": top_k,
            "num_candidates": top_k * 10
        },
        # æ–‡æœ¬æœç´¢ï¼ˆBM25ï¼‰
        "query": {
            "multi_match": {
                "query": query_text,
                "fields": ["title^2", "content", "question", "answer"],
                "type": "best_fields"
            }
        },
        "size": top_k,
        "_source": {
            "excludes": ["embedding"]  # ä¸è¿”å›å‘é‡å­—æ®µï¼ˆå‡å°‘ä¼ è¾“é‡ï¼‰
        }
    }
    
    try:
        # æ‰§è¡Œæ··åˆæœç´¢ï¼ˆES 9.xç‰ˆæœ¬ï¼šç›´æ¥ä¼ é€’å‚æ•°ï¼‰
        response = es.search(
            index=index_name,
            knn=search_body["knn"],
            query=search_body["query"],
            size=search_body["size"],
            _source=search_body["_source"]
        )
        
        # è§£ææœç´¢ç»“æœ
        hits = response['hits']['hits']
        total = response['hits']['total']['value']
        
        print(f"\nâœ“ æ··åˆæœç´¢å®Œæˆï¼")
        print(f"  æ‰¾åˆ° {total} ä¸ªç›¸å…³æ–‡æ¡£")
        print(f"  è¿”å›å‰ {len(hits)} ä¸ªç»“æœ\n")
        
        # æ˜¾ç¤ºç»“æœ
        if hits:
            for i, hit in enumerate(hits, 1):
                score = hit['_score']
                source = hit['_source']
                
                print(f"[ç»“æœ {i}] ç»¼åˆè¯„åˆ†: {score:.4f}")
                print(f"  æ ‡é¢˜: {source.get('title', 'æ— æ ‡é¢˜')}")
                print(f"  æ¥æº: {source.get('source', 'æœªçŸ¥')}")
                print(f"  å†…å®¹: {source.get('content', '')[:200]}...")
                print()
        else:
            print("  âš  æœªæ‰¾åˆ°åŒ¹é…çš„ç»“æœ")
        
        return hits
        
    except Exception as e:
        print(f"âœ— æ··åˆæœç´¢å¤±è´¥: {str(e)}")
        raise


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Elasticsearchæ•°æ®ä¿å­˜å’ŒæŸ¥è¯¢æµ‹è¯•è„šæœ¬')
    parser.add_argument('--mode', type=str, choices=['hybrid'], default='hybrid',
                       help='ä¿å­˜æ¨¡å¼ï¼šhybridï¼ˆæ··åˆç´¢å¼•ï¼ŒåŒæ—¶åŒ…å«å‘é‡å’Œæ–‡æœ¬å­—æ®µï¼‰')
    parser.add_argument('--domain', type=str, choices=['policy', 'system'], default='system',
                       help='åŸŸç±»å‹ï¼ˆé»˜è®¤: systemï¼‰')
    parser.add_argument('--query', type=str, default=None,
                       help='æµ‹è¯•æŸ¥è¯¢æ–‡æœ¬ï¼ˆå¦‚æœä¸æä¾›ï¼Œå°†æç¤ºæ‰‹åŠ¨è¾“å…¥ï¼‰')
    parser.add_argument('--role', type=str, default='å®¢æˆ·ç»ç†',
                       help='ç”¨æˆ·è§’è‰²ï¼ˆé»˜è®¤: å®¢æˆ·ç»ç†ï¼‰')
    parser.add_argument('--test-only', action='store_true',
                       help='ä»…æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•ï¼Œä¸ä¿å­˜æ•°æ®')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("Elasticsearchæ•°æ®ä¿å­˜å’ŒæŸ¥è¯¢æµ‹è¯•è„šæœ¬")
    print("=" * 60)
    print(f"ä¿å­˜æ¨¡å¼: {args.mode}")
    print(f"åŸŸç±»å‹: {args.domain}")
    print(f"ç”¨æˆ·è§’è‰²: {args.role}")
    if args.test_only:
        print("æ¨¡å¼: ä»…æŸ¥è¯¢æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥ä¾èµ–
    if not ELASTICSEARCH_AVAILABLE:
        print("âš  é”™è¯¯: elasticsearchæœªå®‰è£…")
        return
    
    # æ£€æŸ¥Elasticsearchè¿æ¥
    if not test_elasticsearch_connection():
        print("\nâš  é”™è¯¯: Elasticsearchè¿æ¥å¤±è´¥")
        return
    
    # è¿æ¥Elasticsearch
    es = connect_elasticsearch()
    
    # è·å–ç´¢å¼•é…ç½®
    index_config = INDEX_CONFIG.get(args.domain)
    if not index_config:
        print(f"âš  æœªæ‰¾åˆ°åŸŸ {args.domain} çš„ç´¢å¼•é…ç½®")
        return
    
    index_name = index_config['index_name']
    
    # å¦‚æœä¸æ˜¯ä»…æµ‹è¯•æ¨¡å¼ï¼Œæ‰§è¡Œæ•°æ®ä¿å­˜
    if not args.test_only:
        # ä»QAå¯¹JSONæ–‡ä»¶è¯»å–æ•°æ®
        chunks = prepare_chunks_from_qa_pairs(args.domain)
        
        # ä¿å­˜æ··åˆæ•°æ®ï¼ˆå•ä¸ªç´¢å¼•ï¼ŒåŒæ—¶åŒ…å«å‘é‡å’Œæ–‡æœ¬å­—æ®µï¼‰
        if args.mode == 'hybrid':
            # åŠ è½½embeddingæ¨¡å‹
            print("\næ­£åœ¨åŠ è½½embeddingæ¨¡å‹...")
            try:
                model, tokenizer = load_embedding_model(EMBEDDING_MODEL_NAME)
                
                # ç”Ÿæˆå‘é‡
                print("æ­£åœ¨ç”Ÿæˆå‘é‡...")
                texts = [chunk['content'] for chunk in chunks]
                embeddings = generate_embeddings(texts, model, tokenizer, batch_size=4)
                
                # åˆ›å»ºæ··åˆç´¢å¼•ï¼ˆåŒæ—¶æ”¯æŒå‘é‡æœç´¢å’Œæ–‡æœ¬æœç´¢ï¼‰
                create_vector_index(es, index_name, embeddings.shape[1])
                
                # ç´¢å¼•æ–‡æ¡£å’Œå‘é‡ï¼ˆå•ä¸ªç´¢å¼•åŒ…å«å‘é‡å’Œæ–‡æœ¬å­—æ®µï¼‰
                index_documents_with_vectors(es, index_name, chunks, embeddings)
            except Exception as e:
                print(f"âš  å‘é‡ç”Ÿæˆæˆ–ä¿å­˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
    
    # æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•
    print("\n" + "="*60)
    print("æŸ¥è¯¢æµ‹è¯•")
    print("="*60)
    
    # å¦‚æœæ²¡æœ‰æä¾›æŸ¥è¯¢æ–‡æœ¬ï¼Œæç¤ºç”¨æˆ·è¾“å…¥
    if args.query is None or args.query.strip() == '':
        print(f"\næœªæä¾›æŸ¥è¯¢é—®é¢˜ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥ï¼š")
        query_text = input("è¯·è¾“å…¥æŸ¥è¯¢é—®é¢˜: ").strip()
        
        if not query_text:
            print("âš  é”™è¯¯: æŸ¥è¯¢é—®é¢˜ä¸èƒ½ä¸ºç©º")
            return
    else:
        query_text = args.query
    
    # æµ‹è¯•æ··åˆæœç´¢ï¼ˆHybrid Searchï¼šBM25 + å‘é‡ï¼‰
    if args.mode == 'hybrid':
        print("\n" + "="*60)
        print("æµ‹è¯•æ··åˆæœç´¢ï¼ˆHybrid Searchï¼šBM25 + å‘é‡ï¼‰")
        print("="*60)
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            if not NUMPY_AVAILABLE:
                print("âš  é”™è¯¯: numpyæœªå®‰è£…ï¼Œæ— æ³•ç”ŸæˆæŸ¥è¯¢å‘é‡")
                return
            
            query_vector = generate_query_embedding(query_text)
            # æ‰§è¡Œæ··åˆæœç´¢
            test_hybrid_search(es, index_name, query_text, query_vector, top_k=10)
        except Exception as e:
            print(f"âš  æ··åˆæœç´¢æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        # å¯é€‰ï¼šå•ç‹¬æµ‹è¯•æ–‡æœ¬æœç´¢å’Œå‘é‡æœç´¢
        print("\n" + "="*60)
        print("é¢å¤–æµ‹è¯•ï¼šæ–‡æœ¬æœç´¢ï¼ˆBM25ï¼‰")
        print("="*60)
        try:
            test_text_search(es, index_name, query_text, top_k=5)
        except Exception as e:
            print(f"âš  æ–‡æœ¬æœç´¢æµ‹è¯•å¤±è´¥: {e}")
        
        print("\n" + "="*60)
        print("é¢å¤–æµ‹è¯•ï¼šå‘é‡æœç´¢ï¼ˆKNNï¼‰")
        print("="*60)
        try:
            test_vector_search(query_text, args.domain, args.role, top_k=5)
        except Exception as e:
            print(f"âš  å‘é‡æœç´¢æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n" + "="*60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("="*60)


if __name__ == "__main__":
    main()
