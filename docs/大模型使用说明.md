# 项目大模型使用说明

## 一、概述

本项目是一个智能信贷业务辅助系统，使用了多种大模型和机器学习模型来实现不同的功能。本文档详细说明项目中使用的所有模型及其用途。

---

## 二、模型列表

### 2.1 Embedding模型（向量化模型）

#### 模型名称
- **Qwen3-Embedding-0.6B**（默认）
- 模型路径：`models/embedding/qwen/Qwen3-Embedding-0___6B/`

#### 主要用途
1. **查询向量化**：将用户查询转换为向量表示
2. **文档向量化**：将政策文档、系统功能文档转换为向量
3. **向量检索**：支持Elasticsearch向量搜索，实现语义检索

#### 使用位置
- `src/rag/query.py` - RAG查询模块
- `scripts/rag/build_vector_db.py` - 向量库构建脚本

#### 配置方式
```python
# 环境变量配置
EMBEDDING_MODEL = 'Qwen/Qwen3-Embedding-0.6B'  # 默认值
```

#### 技术特点
- 模型大小：0.6B参数
- 向量维度：1024维
- 支持中文语义理解
- 本地部署，无需API调用

---

### 2.2 大语言模型（LLM）

项目支持两种LLM调用模式：**本地部署**和**API调用**（百炼API）

#### 2.2.1 本地LLM模型

##### 模型名称
- **Qwen2.5-7B-Instruct**（默认）
- 通过Hugging Face或ModelScope加载

##### 主要用途
1. **Query改写**：将用户口语化问题改写为规范查询
2. **意图识别**：使用CoT思维链推理识别用户意图
3. **RAG答案生成**：基于检索到的文档生成最终答案
4. **QA对生成**：从文档内容生成问答对

##### 使用位置
- `src/rag/query.py` - RAG查询和Query改写
- `src/intent/classification.py` - 意图识别
- `scripts/qa_generation/generate_qa_from_docs.py` - QA对生成

##### 配置方式
```python
# 环境变量配置
LLM_MODE = 'local'  # 使用本地模型
LOCAL_MODEL_PATH = 'Qwen/Qwen2.5-7B-Instruct'  # 默认值
```

##### 技术特点
- 模型大小：7B参数
- 支持指令跟随（Instruct格式）
- 本地部署，数据隐私保护
- 支持GPU/CPU推理

#### 2.2.2 百炼API模型

##### 模型名称
- **qwen-plus**（默认，可通过环境变量配置）

##### 主要用途
与本地LLM模型相同，但通过API调用：
1. Query改写
2. 意图识别
3. RAG答案生成
4. QA对生成

##### 使用位置
- `src/rag/query.py`
- `src/intent/classification.py`
- `scripts/qa_generation/generate_qa_from_docs.py`

##### 配置方式
```python
# 环境变量配置
LLM_MODE = 'bailian'  # 使用百炼API
DASHSCOPE_API_KEY = 'your-api-key'  # 必需
BAILIAN_MODEL = 'qwen-plus'  # 默认值，可配置为其他模型
```

##### 技术特点
- 云端API调用，无需本地部署
- 支持多种模型选择（qwen-plus, qwen-max等）
- 需要网络连接和API密钥
- 按token计费

---

### 2.3 机器学习模型（非大模型）

#### 模型名称
- **LightGBM**（梯度提升决策树）

##### 主要用途
1. **客户贷款金额预测**：预测客户未来n个月的贷款金额
2. **特征重要性分析**：分析影响贷款金额的关键因素

##### 使用位置
- `src/prediction/model_train.py` - 模型训练
- `src/prediction/model_predict_test.py` - 模型预测

##### 模型特点
- 回归模型（预测连续值）
- 基于树模型，无需特征标准化
- 支持特征重要性分析
- 模型文件：`models/lightgbm/lightgbm_model.txt`

---

## 三、模型使用场景

### 3.1 RAG系统流程

```
用户查询
  ↓
[Query改写] ← LLM（本地/API）
  ↓
[向量化] ← Embedding模型
  ↓
[向量检索] ← Elasticsearch
  ↓
[答案生成] ← LLM（本地/API）
  ↓
返回答案
```

**使用的模型**：
- Embedding模型：查询和文档向量化
- LLM模型：Query改写和答案生成

### 3.2 意图识别流程

```
用户问题
  ↓
[意图识别] ← LLM（CoT思维链推理）
  ↓
路由决策
  ├─→ RAG（政策/系统）
  └─→ 预测模块
```

**使用的模型**：
- LLM模型：意图识别和路由决策

### 3.3 QA对生成流程

```
文档内容
  ↓
[文本切分]（不使用LLM）
  ↓
[QA对生成] ← LLM（本地/API）
  ↓
[向量化] ← Embedding模型
  ↓
[入库] ← Elasticsearch
```

**使用的模型**：
- LLM模型：生成问答对
- Embedding模型：QA对向量化

### 3.4 客户预测流程

```
客户数据
  ↓
[特征提取]
  ↓
[模型预测] ← LightGBM
  ↓
预测结果
```

**使用的模型**：
- LightGBM：贷款金额预测

---

## 四、模型配置说明

### 4.1 环境变量配置

创建 `.env` 文件（不要提交到Git）：

```env
# Embedding模型配置
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B

# LLM模式配置（'local' 或 'bailian'）
LLM_MODE=bailian

# 本地LLM模型配置
LOCAL_MODEL_PATH=Qwen/Qwen2.5-7B-Instruct

# 百炼API配置
DASHSCOPE_API_KEY=your-api-key-here
BAILIAN_MODEL=qwen-plus

# 其他配置
FORCE_CPU=false  # 强制使用CPU（本地模型）
ANSWER_MAX_LENGTH=1000  # 答案最大长度
```

### 4.2 模型切换

#### 切换LLM模式
```bash
# 使用本地模型
export LLM_MODE=local

# 使用百炼API
export LLM_MODE=bailian
export DASHSCOPE_API_KEY=your-api-key
```

#### 切换Embedding模型
```bash
export EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
```

---

## 五、模型调用统计

### 5.1 各模块的模型使用

| 模块 | 使用的模型 | 调用频率 | 说明 |
|------|-----------|---------|------|
| Query改写 | LLM | 每次查询 | 将用户问题改写为规范查询 |
| 意图识别 | LLM | 每次查询 | CoT思维链推理识别意图 |
| 向量检索 | Embedding | 每次查询 | 查询和文档向量化 |
| 答案生成 | LLM | 每次查询 | 基于检索结果生成答案 |
| QA对生成 | LLM | 数据预处理 | 批量生成问答对 |
| 客户预测 | LightGBM | 按需调用 | 预测贷款金额 |

### 5.2 Token监控

项目实现了完整的Token监控系统（`src/utils/llm_monitor.py`），记录：
- 每次LLM调用的token消耗
- 输入token数（prompt_tokens）
- 输出token数（completion_tokens）
- 总token数
- 调用耗时

---

## 六、模型文件位置

### 6.1 Embedding模型
```
models/embedding/qwen/Qwen3-Embedding-0___6B/
├── model.safetensors
├── config.json
├── tokenizer.json
└── ...
```

### 6.2 本地LLM模型
- 通过Hugging Face或ModelScope自动下载
- 缓存位置：`~/.cache/huggingface/` 或 `./models/`

### 6.3 LightGBM模型
```
models/lightgbm/
├── lightgbm_model.txt      # 训练好的模型
├── encoders.pkl            # 特征编码器
├── feature_names.pkl       # 特征名称列表
├── metrics.pkl             # 评估指标
└── feature_importance.csv  # 特征重要性
```

---

## 七、模型依赖

### 7.1 Python包依赖

```bash
# Embedding模型
pip install modelscope transformers torch

# 本地LLM模型
pip install transformers torch

# 百炼API
pip install dashscope

# LightGBM
pip install lightgbm scikit-learn
```

### 7.2 硬件要求

- **Embedding模型**：CPU即可，GPU可选
- **本地LLM（7B）**：建议16GB+内存，GPU可选（8GB+显存）
- **LightGBM**：CPU即可

---

## 八、注意事项

### 8.1 数据隐私
- 本地模型：数据不离开本地环境
- API模型：数据会发送到云端，注意敏感信息

### 8.2 成本控制
- 百炼API按token计费，建议监控token使用
- 本地模型需要GPU资源，注意显存占用

### 8.3 模型更新
- Embedding模型：通过ModelScope自动下载
- LLM模型：通过Hugging Face/ModelScope下载
- LightGBM模型：需要重新训练更新

---

## 九、总结

### 9.1 模型使用汇总

| 模型类型 | 模型名称 | 用途 | 部署方式 |
|---------|---------|------|---------|
| Embedding | Qwen3-Embedding-0.6B | 文本向量化 | 本地部署 |
| LLM | Qwen2.5-7B-Instruct | 文本生成、理解 | 本地/API |
| LLM | qwen-plus（API） | 文本生成、理解 | 云端API |
| ML | LightGBM | 贷款金额预测 | 本地部署 |

### 9.2 核心功能

1. **RAG检索增强生成**：使用Embedding + LLM实现智能问答
2. **意图识别与路由**：使用LLM进行问题理解和路由
3. **客户预测分析**：使用LightGBM进行贷款金额预测

---

**文档版本**: v1.0  
**最后更新**: 2025-01-XX  
**维护者**: 项目开发团队

